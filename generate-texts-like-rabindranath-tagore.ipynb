{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004315,"sourceType":"datasetVersion","datasetId":1784343}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Generate Texts Like Rabindranath Tagore**","metadata":{}},{"cell_type":"code","source":"# lets start...","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:20:33.209657Z","iopub.execute_input":"2024-07-21T20:20:33.210472Z","iopub.status.idle":"2024-07-21T20:20:33.214587Z","shell.execute_reply.started":"2024-07-21T20:20:33.210433Z","shell.execute_reply":"2024-07-21T20:20:33.213558Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nNatural Language Processing (NLP) is a crucial field in artificial intelligence that deals with the interaction between computers and human language. One of the fascinating tasks in NLP is Language Modelling, where the goal is to predict the next word in a sequence of words. This notebook aims to explore Language Modelling or Sentence Completion Task using a Complex BiLSTM model on a Bengali Texts written by Rabindranath Tagore, One of the greatest works available in Bengali Literature. \n\nThis Notebook is Completely Beginner-friendly and helpful to people who are interested in Enhancing NLP Tasks in Indian Languages.\n\n## Objective\n\nMy Objective is to predict the next word in a given sequence from a corpus of Rabindranath Tagore's writings for Sentence Completion or Text-Generation. The dataset consists of approximately 1 lakh words, and the challenge is to leverage this dataset to train a BiLSTM model for accurate next word prediction and Sentence Completion.\n\n## Dataset\n\nThe dataset used in this notebook comprises essays and novels written by Rabindranath Tagore. It includes around 1 lakh words, providing a rich linguistic resource on Bengali Language for training and evaluating the model.\n\n<!-- ## Methodology\n\n1. **Data Preprocessing**: Cleaning and preparing the text data for training the model.\n2. **Model Building**: Constructing a BiLSTM model suitable for next word prediction.\n3. **Training**: Training the model on the dataset using appropriate hyperparameters.\n4. **Evaluation**: Assessing the model's performance using accuracy metrics such as top-1, top-2, and top-5 accuracy.\n\n## Acknowledgements\n\nI would like to express my gratitude to the following:\n- The Kaggle community for providing a platform to share and collaborate on data science projects.\n- The creators of the dataset for making it publicly available.\n- Rabindranath Tagore, whose literary works have been an inspiration and a source of rich textual data for this project.\n\n## Conclusion\n\nThis notebook demonstrates the process of building and evaluating a BiLSTM model for next word prediction using a dataset of essays by Rabindranath Tagore. The findings and insights from this project can contribute to further research and applications in the field of NLP and text generation. -->\n\n\n- \n- \n- \n\n\n---\n\n\nLet's dive into the data and start our journey into the world of Text Generation On Bengali Language using Rabindranath Tagore's Writings!\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledgements\n\nI would like to express my gratitude to the following:\n- The Kaggle community for providing a platform to share and collaborate on data science projects.\n- The creators of the dataset for making it publicly available.\n- Rabindranath Tagore, whose literary works have been an inspiration and a source of rich textual data for this project.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nFor my Work I have used The following resources for Preprocessing, Model Development and Trainning\n- [Bengali_Text_Preprocessing_for_Language_Modeling](https://www.kaggle.com/code/sayankr007/bengali-text-preprocessing-for-language-modeling)  by  [sayankr007](https://www.kaggle.com/sayankr007)\n- [Next_Word_Prediction_using_LSTM](https://www.youtube.com/watch?v=NYUIxVQa7TE)  by  [CampusX](https://learnwith.campusx.in/)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Methodology","metadata":{}},{"cell_type":"markdown","source":"## Necessary Imports","metadata":{}},{"cell_type":"code","source":"# general libraries\nimport os\nimport random\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:20:50.446308Z","iopub.execute_input":"2024-07-21T20:20:50.446632Z","iopub.status.idle":"2024-07-21T20:20:50.809108Z","shell.execute_reply.started":"2024-07-21T20:20:50.446608Z","shell.execute_reply":"2024-07-21T20:20:50.808152Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# ML/DL Libraries\nfrom nltk.tokenize import word_tokenize\n\nimport torch\nimport torchtext\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.functional import one_hot\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:20:51.799520Z","iopub.execute_input":"2024-07-21T20:20:51.799995Z","iopub.status.idle":"2024-07-21T20:20:57.545128Z","shell.execute_reply.started":"2024-07-21T20:20:51.799964Z","shell.execute_reply":"2024-07-21T20:20:57.544344Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Loading and Cleaning","metadata":{}},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/input/complete-works-of-rabindranath-tagore/txt/essay.txt\"\n\nwith open(file_path, 'r') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:21:01.642736Z","iopub.execute_input":"2024-07-21T20:21:01.643300Z","iopub.status.idle":"2024-07-21T20:21:02.119425Z","shell.execute_reply.started":"2024-07-21T20:21:01.643270Z","shell.execute_reply":"2024-07-21T20:21:02.118433Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Sample Raw Dataset..\ntext[0:200]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:21:03.954214Z","iopub.execute_input":"2024-07-21T20:21:03.955053Z","iopub.status.idle":"2024-07-21T20:21:03.962485Z","shell.execute_reply.started":"2024-07-21T20:21:03.955015Z","shell.execute_reply":"2024-07-21T20:21:03.961509Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'আশ্রমের রূপ ও বিকাশ ২\\nশিলাইদহে পদ্মাতীরে সাহিত্যচর্চা নিয়ে নিভৃতে বাস করতুম। একটা সৃষ্টির সংকল্প নিয়ে সেখান থেকে এলেম শান্তিনিকেতনের প্রান্তরে।\\nতখন আশ্রমের পরিধি ছিল ছোটো। তার দক্ষিণ সীমানায় দীর্ঘ সার'"},"metadata":{}}]},{"cell_type":"markdown","source":"###  Cleaning The Data","metadata":{}},{"cell_type":"markdown","source":"### Remove Expressions (WhiteSpaces and Punctuations)\n\n*We are using Python's Regular Expressions RE library for this..*","metadata":{}},{"cell_type":"code","source":"# replacing all punctuation symbols and whitespaces with a single whitepace\n\nwhitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\nbangla_fullstop = u\"\\u0964\"  # unicode code for bengali fullstop\npunctSeq = u\"['\\\"“”‘’]+|[.…-]+|[:;]+\"\nbengali_numeral_pattern = r'[১-৯০]+' # Regular expression pattern to match Bengali numerals\n\n\n\ncorpus = re.sub(bengali_numeral_pattern,\" \",text)   # Remove individual Bengali numerals..\ncorpus = re.sub(r'\\s*\\d+\\s*', ' ', corpus)          # Remove all possible sequences of Bengali numerals..\n\ncorpus = re.sub('\\n',\" \",corpus)                                # remove all '\\n' symbols\ncorpus = re.sub(punctSeq,\" \",corpus)                            # remove all punctuations..\ncorpus = whitespace.sub(\" \",corpus).strip()                     # the strip() method to remove any leading or trailing whitespace from the resulting string","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:21:07.519453Z","iopub.execute_input":"2024-07-21T20:21:07.519822Z","iopub.status.idle":"2024-07-21T20:21:11.977212Z","shell.execute_reply.started":"2024-07-21T20:21:07.519793Z","shell.execute_reply":"2024-07-21T20:21:11.976400Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Sample Corpus after Pre-processing\ncorpus[0:500]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:21:11.978656Z","iopub.execute_input":"2024-07-21T20:21:11.978954Z","iopub.status.idle":"2024-07-21T20:21:11.984656Z","shell.execute_reply.started":"2024-07-21T20:21:11.978929Z","shell.execute_reply":"2024-07-21T20:21:11.983739Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'আশ্রমের রূপ ও বিকাশ শিলাইদহে পদ্মাতীরে সাহিত্যচর্চা নিয়ে নিভৃতে বাস করতুম। একটা সৃষ্টির সংকল্প নিয়ে সেখান থেকে এলেম শান্তিনিকেতনের প্রান্তরে। তখন আশ্রমের পরিধি ছিল ছোটো। তার দক্ষিণ সীমানায় দীর্ঘ সার বাঁধা শালগাছ। মাধবীলতা বিতানে প্রবেশের দ্বার। পিছনে পুব দিকে আমবাগান, পশ্চিম দিকে কোথাও বা তাল, কোথাও বা জাম, কোথাও বা ঝাউ, ইতস্তত গুটিকয়েক নারকেল। উত্তরপশ্চিম প্রান্তে প্রাচীন দুটি ছাতিমের তলায় মার্বেল পাথরে বাঁধানো একটি নিরলংকৃত বেদী। তার সামনে গাছের আড়াল নেই, দিগন্ত পর্যন্ত অবারিত মাঠ, সে মাঠে তখন'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Text PreProcessing\n\n**Note** : \n*The Data Cleaning and Text-Preprocessing Steps on the Same data have been discussed in details with tutorial in the follwing link -> [Bengali_Text_Preprocessing_for_Language_Modeling](https://www.kaggle.com/code/sayankr007/bengali-text-preprocessing-for-language-modeling)*","metadata":{}},{"cell_type":"markdown","source":"### Tokenization\n\n*We are using NLTK Libraries Word_Tokenizer for this task..*","metadata":{}},{"cell_type":"code","source":"# Tokenize sentences based on the Bengali fullstop symbol\nsentences = corpus.split(bangla_fullstop)\n\n# Remove empty strings and add the period symbol back to each sentence\nsentences = [sentence.strip() + \" \" + bangla_fullstop for sentence in sentences if sentence.strip()]\n\n# Tokenize each sentence into words\ndoc = [word_tokenize(sentence) for sentence in sentences]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:21:16.832330Z","iopub.execute_input":"2024-07-21T20:21:16.832955Z","iopub.status.idle":"2024-07-21T20:21:40.497416Z","shell.execute_reply.started":"2024-07-21T20:21:16.832927Z","shell.execute_reply":"2024-07-21T20:21:40.496639Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(\"Before Tokenization : \\n\", corpus[-386:],\"\\n\")\nprint(\"After Tokenization : \\n\", doc[-3:])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:02.809352Z","iopub.execute_input":"2024-07-21T20:23:02.809722Z","iopub.status.idle":"2024-07-21T20:23:02.815417Z","shell.execute_reply.started":"2024-07-21T20:23:02.809694Z","shell.execute_reply":"2024-07-21T20:23:02.814442Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Before Tokenization : \n কিন্তু আমরা তো বিজ্ঞানী নই, বুঝতে পারি নে হঠাৎ অঙ্কের আরম্ভ হয় কোথা থেকে, একেবারে শেষই বা হয় কোন্ খানে। সম্পূর্ণ সংঘটিত বিশ্বকে নিয়ে হঠাৎ কালের আরম্ভ হল আর সদ্যোলুপ্ত বিশ্বের সঙ্গে কালের সম্পূর্ণ অন্ত হবে, আমাদের বুদ্ধিতে এর কিনারা পাই নে। বিজ্ঞানী বলবেন, বুদ্ধির কথা এখানে আসছে না, এ হল গণনার কথা সে গণনা বর্তমান ঘটনাধারার উপরে প্রতিষ্ঠিত এর আদি অন্তে যদি অন্ধকার দেখি তা হলে উপায় নেই। \n\nAfter Tokenization : \n [['কিন্তু', 'আমরা', 'তো', 'বিজ্ঞানী', 'নই', ',', 'বুঝতে', 'পারি', 'নে', 'হঠাৎ', 'অঙ্কের', 'আরম্ভ', 'হয়', 'কোথা', 'থেকে', ',', 'একেবারে', 'শেষই', 'বা', 'হয়', 'কোন্', 'খানে', '।'], ['সম্পূর্ণ', 'সংঘটিত', 'বিশ্বকে', 'নিয়ে', 'হঠাৎ', 'কালের', 'আরম্ভ', 'হল', 'আর', 'সদ্যোলুপ্ত', 'বিশ্বের', 'সঙ্গে', 'কালের', 'সম্পূর্ণ', 'অন্ত', 'হবে', ',', 'আমাদের', 'বুদ্ধিতে', 'এর', 'কিনারা', 'পাই', 'নে', '।'], ['বিজ্ঞানী', 'বলবেন', ',', 'বুদ্ধির', 'কথা', 'এখানে', 'আসছে', 'না', ',', 'এ', 'হল', 'গণনার', 'কথা', 'সে', 'গণনা', 'বর্তমান', 'ঘটনাধারার', 'উপরে', 'প্রতিষ্ঠিত', 'এর', 'আদি', 'অন্তে', 'যদি', 'অন্ধকার', 'দেখি', 'তা', 'হলে', 'উপায়', 'নেই', '।']]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Total length of the Document -> \", len(doc))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:08.534735Z","iopub.execute_input":"2024-07-21T20:23:08.535107Z","iopub.status.idle":"2024-07-21T20:23:08.540610Z","shell.execute_reply.started":"2024-07-21T20:23:08.535078Z","shell.execute_reply":"2024-07-21T20:23:08.539464Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Total length of the Document ->  98304\n","output_type":"stream"}]},{"cell_type":"code","source":"# Working with Half Data due to limited resource\n# process and train the model twice, with half data each time...\n\ndoc = doc[:50000]\n# doc = doc[50000: ]\n\n# NOTE : You can train the whole dataset if enough Computational Resources \n#        are available to you..","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:13.406759Z","iopub.execute_input":"2024-07-21T20:23:13.407585Z","iopub.status.idle":"2024-07-21T20:23:13.442050Z","shell.execute_reply.started":"2024-07-21T20:23:13.407554Z","shell.execute_reply":"2024-07-21T20:23:13.441097Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Build Vocab from Iterator","metadata":{}},{"cell_type":"code","source":"word_vocab = torchtext.vocab.build_vocab_from_iterator(\n    doc,\n    min_freq=1,\n    specials=['<pad>', '<unk>'],  # <pad> for padding sequence to same length(MAX)\n                                  # <unk> for unknown out-of-vocabulary words\n    special_first=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:26.165765Z","iopub.execute_input":"2024-07-21T20:23:26.166424Z","iopub.status.idle":"2024-07-21T20:23:26.619656Z","shell.execute_reply.started":"2024-07-21T20:23:26.166390Z","shell.execute_reply":"2024-07-21T20:23:26.618670Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":" **`get_itos`**: *stands for \"index-to-string\". The method returns a list where the indices in the list correspond to the numerical indices used in your model, and the values at those indices are the actual string representations (tokens).*","metadata":{}},{"cell_type":"code","source":"# This Step is very very Important for Converting Words into their respective index,\n# Creating \"idx-to-word\" and \"word-to-idx\" Dictionaries makes the searching easier \n# by reducing the time by several minutes even on a very large corpus like ours..\n\n\n# Create the word-to-index map\nword_to_index = {token: idx for idx, token in enumerate(word_vocab.get_itos())}\n\n# Create the index-to-word map\nindex_to_word = {idx: token for idx, token in enumerate(word_vocab.get_itos())}","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:29.568785Z","iopub.execute_input":"2024-07-21T20:23:29.569624Z","iopub.status.idle":"2024-07-21T20:23:29.641731Z","shell.execute_reply.started":"2024-07-21T20:23:29.569590Z","shell.execute_reply":"2024-07-21T20:23:29.640772Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Sample list of Vocabulary\nlist(word_vocab.get_itos())[:10]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:31.286955Z","iopub.execute_input":"2024-07-21T20:23:31.287547Z","iopub.status.idle":"2024-07-21T20:23:31.319041Z","shell.execute_reply.started":"2024-07-21T20:23:31.287517Z","shell.execute_reply":"2024-07-21T20:23:31.318215Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['<pad>', '<unk>', '।', ',', 'না', 'যে', 'এই', 'আমাদের', 'করিয়া', 'করে']"},"metadata":{}}]},{"cell_type":"code","source":"word_vocab_total_words = len(word_vocab)\n\nprint(\"Word Vocab Length\", word_vocab_total_words)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:32.475146Z","iopub.execute_input":"2024-07-21T20:23:32.475779Z","iopub.status.idle":"2024-07-21T20:23:32.481007Z","shell.execute_reply.started":"2024-07-21T20:23:32.475745Z","shell.execute_reply":"2024-07-21T20:23:32.479907Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Word Vocab Length 77446\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Converting to n-grams\n\nn_grams of Variable Length\n\n**Source Data** : *I Am Learning Artificial Intelligence*\n\n- *-----  **X**  -----------------------------  **y**  -----*\n\n\n- *I-----------------------------------Am*\n- *I Am------------------------------Learning*\n- *I Am Learning--------------------Artificial*\n- *I Am Learning Artificial----------Intelligence*","metadata":{}},{"cell_type":"code","source":"# function for n-grams of variable length:\n\ndef seq2grams(sentences):\n    n_grams = []\n    for sentence in tqdm(sentences):            # for each sentence in the corpus\n        for i in range(1, len(sentence)):  # from [1st] word, [1st,2nd] word, [1st,2nd,3rd] word upto last word inde \n            sequence = sentence[:i+1]      # make sequences [1,2], [1,2,3], [1,2,3,4] and so on\n            n_grams.append(sequence)        # add the sequence to the main array\n    return n_grams\n\ndataset = seq2grams(doc)\n\n# sample dataset after n-gram\nprint(dataset[:5])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:35.096510Z","iopub.execute_input":"2024-07-21T20:23:35.097141Z","iopub.status.idle":"2024-07-21T20:23:36.569727Z","shell.execute_reply.started":"2024-07-21T20:23:35.097109Z","shell.execute_reply":"2024-07-21T20:23:36.568854Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 50000/50000 [00:01<00:00, 34180.42it/s]","output_type":"stream"},{"name":"stdout","text":"[['আশ্রমের', 'রূপ'], ['আশ্রমের', 'রূপ', 'ও'], ['আশ্রমের', 'রূপ', 'ও', 'বিকাশ'], ['আশ্রমের', 'রূপ', 'ও', 'বিকাশ', 'শিলাইদহে'], ['আশ্রমের', 'রূপ', 'ও', 'বিকাশ', 'শিলাইদহে', 'পদ্মাতীরে']]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Adding Random  `<unk>` Tokens\n\n*This is a cruicial step used for Efficient Predictions where the test set may include many Out-Of-Dictionary words.. so, the trainning set should include `<unk>` tokens.*","metadata":{}},{"cell_type":"code","source":"# Add Random <unk> tokens to let the model handle <unk> tokens\ndef add_random_unk_tokens(ngram):\n    for idx, word in enumerate(ngram[:-1]):\n        if random.uniform(0, 1) < 0.1:\n            ngram[idx] = '<unk>'\n    return ngram\n\n\ndataset_unk = []\nfor data in dataset:\n    dataset_unk.append(add_random_unk_tokens(data))\n    \n\n# check whether the dataset includes <unk> tokens or not..    \nprint(any('<unk>' in data for data in dataset_unk))\n\n# length of the dataset\nprint(len(dataset_unk))\n\n# sample dataset after adding random <unk> tokens\nprint(dataset_unk[34:43])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:23:40.158607Z","iopub.execute_input":"2024-07-21T20:23:40.158998Z","iopub.status.idle":"2024-07-21T20:23:47.485683Z","shell.execute_reply.started":"2024-07-21T20:23:40.158968Z","shell.execute_reply":"2024-07-21T20:23:47.484772Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"True\n865889\n[['মাধবীলতা', 'বিতানে', '<unk>', 'দ্বার'], ['<unk>', 'বিতানে', 'প্রবেশের', '<unk>', '।'], ['<unk>', 'পুব'], ['পিছনে', 'পুব', 'দিকে'], ['পিছনে', '<unk>', 'দিকে', 'আমবাগান'], ['<unk>', 'পুব', 'দিকে', '<unk>', ','], ['পিছনে', 'পুব', '<unk>', 'আমবাগান', ',', 'পশ্চিম'], ['পিছনে', '<unk>', 'দিকে', 'আমবাগান', '<unk>', 'পশ্চিম', 'দিকে'], ['পিছনে', 'পুব', 'দিকে', 'আমবাগান', ',', 'পশ্চিম', 'দিকে', 'কোথাও']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Conversion of Word to Tokens(numbers)","metadata":{}},{"cell_type":"code","source":"# converts the Words in the dataset into Numbers... using\n# \"word-to-index\" Mapping\ndef text_to_numerical_sequence(tokenized_text):\n    tokens_list = []\n    if tokenized_text[-1] in word_to_index:\n        for token in tokenized_text[:-1]:\n            num_token = word_to_index[token] if token in word_to_index  else word_to_index['<unk>']\n            tokens_list.append(num_token)\n        num_token = word_to_index[tokenized_text[-1]]\n        tokens_list.append(num_token)\n        return tokens_list\n    return None\n\n\n# Efficiently create data list without redundant calls\ndata = [seq for seq in (text_to_numerical_sequence(sequence) for sequence in dataset_unk)]\n\nprint(f'Total input sequences: {len(data)}')\nprint('Sample Dataset :\\n', data[7:9])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:24:32.018830Z","iopub.execute_input":"2024-07-21T20:24:32.019204Z","iopub.status.idle":"2024-07-21T20:24:37.104006Z","shell.execute_reply.started":"2024-07-21T20:24:32.019176Z","shell.execute_reply":"2024-07-21T20:24:37.103097Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Total input sequences: 865889\nSample Dataset :\n [[1172, 386, 1, 860, 5746, 19000, 16429, 113, 4380], [1172, 386, 17, 860, 5746, 19000, 16429, 113, 4380, 664]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Creation of Features and Labels\n\n*In each n-gram last word will be the `LABEL` and all previous words will be the `FEATURE`*","metadata":{}},{"cell_type":"code","source":"X = [sequence[:-1] for sequence in data]\ny = [sequence[-1] for sequence in data]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:24:37.105864Z","iopub.execute_input":"2024-07-21T20:24:37.106298Z","iopub.status.idle":"2024-07-21T20:24:38.694447Z","shell.execute_reply.started":"2024-07-21T20:24:37.106263Z","shell.execute_reply":"2024-07-21T20:24:38.693470Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# this will be the maximum size of each token\nlongest_sequence_feature = max(len(sequence) for sequence in X)\nlongest_sequence_feature ","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:24:42.349465Z","iopub.execute_input":"2024-07-21T20:24:42.350308Z","iopub.status.idle":"2024-07-21T20:24:42.438477Z","shell.execute_reply.started":"2024-07-21T20:24:42.350267Z","shell.execute_reply":"2024-07-21T20:24:42.437583Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"464"},"metadata":{}}]},{"cell_type":"markdown","source":"### Padding the Features\n\n**Padding The Features to a Fixed Length (`longest_sequence_feature`) is because our model is expecting same size for each `input` and each n-gram is of different length..**\n\n*`F.pad` function is a utility function which is part of torch.nn.functional module,used for padding tensors.*","metadata":{}},{"cell_type":"code","source":"padded_X = [F.pad(torch.tensor(sequence), (longest_sequence_feature - len(sequence),0), value=0) for sequence in X]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:24:45.567550Z","iopub.execute_input":"2024-07-21T20:24:45.567932Z","iopub.status.idle":"2024-07-21T20:25:04.097719Z","shell.execute_reply.started":"2024-07-21T20:24:45.567901Z","shell.execute_reply":"2024-07-21T20:25:04.096731Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"padded_X = torch.stack(padded_X)\ny = torch.tensor(y)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:04.099263Z","iopub.execute_input":"2024-07-21T20:25:04.099549Z","iopub.status.idle":"2024-07-21T20:25:07.935183Z","shell.execute_reply.started":"2024-07-21T20:25:04.099525Z","shell.execute_reply":"2024-07-21T20:25:07.934163Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# sample padded data\npadded_X[1]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:23.401002Z","iopub.execute_input":"2024-07-21T20:25:23.401681Z","iopub.status.idle":"2024-07-21T20:25:23.422432Z","shell.execute_reply.started":"2024-07-21T20:25:23.401646Z","shell.execute_reply":"2024-07-21T20:25:23.421555Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0, 1172,  386])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Creation of Dataset","metadata":{}},{"cell_type":"code","source":"# create tensordataset using the feature and labels\ndata = TensorDataset(padded_X, y)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:28.082656Z","iopub.execute_input":"2024-07-21T20:25:28.083261Z","iopub.status.idle":"2024-07-21T20:25:28.173076Z","shell.execute_reply.started":"2024-07-21T20:25:28.083231Z","shell.execute_reply":"2024-07-21T20:25:28.172079Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.80 * len(data))\nval_size = int(0.10 * len(data))\ntest_size = len(data) - (train_size + val_size)\n\nprint(train_size, val_size, test_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:28.862356Z","iopub.execute_input":"2024-07-21T20:25:28.863171Z","iopub.status.idle":"2024-07-21T20:25:28.867831Z","shell.execute_reply.started":"2024-07-21T20:25:28.863141Z","shell.execute_reply":"2024-07-21T20:25:28.866893Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"692711 86588 86590\n","output_type":"stream"}]},{"cell_type":"code","source":"# Spliting The Data into Train, Validation and Test Sets.... (8 : 1 : 1)\n\ntrain_data, valid_data, test_data = random_split(data, [train_size, val_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:32.904212Z","iopub.execute_input":"2024-07-21T20:25:32.905261Z","iopub.status.idle":"2024-07-21T20:25:32.986051Z","shell.execute_reply.started":"2024-07-21T20:25:32.905228Z","shell.execute_reply":"2024-07-21T20:25:32.985067Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Data","metadata":{}},{"cell_type":"code","source":"batch_size=512\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:36.079746Z","iopub.execute_input":"2024-07-21T20:25:36.080425Z","iopub.status.idle":"2024-07-21T20:25:36.085740Z","shell.execute_reply.started":"2024-07-21T20:25:36.080393Z","shell.execute_reply":"2024-07-21T20:25:36.084745Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Model Development\n\n*We are going to develop a Bi-directional LSTM due to its*\n- Long-term Memory Support\n- Analyzing Sequence from Both End","metadata":{}},{"cell_type":"code","source":"class My_BiLSTM(nn.Module):\n    def __init__(self, word_vocab_total_words, embedding_dim, hidden_dim, num_layers):\n        super(My_BiLSTM, self).__init__()\n        self.embedding = nn.Embedding(word_vocab_total_words, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(hidden_dim * 2, word_vocab_total_words)\n\n    def forward(self, x):\n        x = x.to(self.embedding.weight.device)\n        embedded = self.embedding(x)\n        lstm_out, _ = self.lstm(embedded)\n        lstm_out = self.dropout(lstm_out)\n\n        \n        # Since the LSTM is bidirectional, we concatenate the last hidden state of the forward direction\n        # and the first hidden state of the backward direction before passing it to the fully connected layer\n        # For batch_first=True, the last timestep of the forward direction is lstm_out[:, -1, :hidden_dim]\n        # and the first timestep of the backward direction is lstm_out[:, 0, hidden_dim:]\n        forward_last = lstm_out[:, -1, :self.lstm.hidden_size]\n        backward_first = lstm_out[:, 0, self.lstm.hidden_size:]\n        \n        output = self.fc(torch.cat((forward_last, backward_first), dim=1))\n        \n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:40.899339Z","iopub.execute_input":"2024-07-21T20:25:40.900114Z","iopub.status.idle":"2024-07-21T20:25:40.915464Z","shell.execute_reply.started":"2024-07-21T20:25:40.900087Z","shell.execute_reply":"2024-07-21T20:25:40.914427Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Setting Up","metadata":{}},{"cell_type":"code","source":"def set_seed(seed):\n    # Set the seed for generating random numbers in PyTorch\n    torch.manual_seed(seed)\n    \n    # If using GPU, set the seed for CUDA\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # Ensure that the PyTorch operations are deterministic on the GPU for reproducibility\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \n    # Set the seed for generating random numbers in Python\n    random.seed(seed)\n    \n    # Set the seed for generating random numbers in NumPy\n    np.random.seed(seed)\n\n    \nseed = 42\nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:44.523135Z","iopub.execute_input":"2024-07-21T20:25:44.523485Z","iopub.status.idle":"2024-07-21T20:25:44.572514Z","shell.execute_reply.started":"2024-07-21T20:25:44.523458Z","shell.execute_reply":"2024-07-21T20:25:44.571561Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# setting the device to \"cuda\" if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:25:45.924998Z","iopub.execute_input":"2024-07-21T20:25:45.925611Z","iopub.status.idle":"2024-07-21T20:25:45.931798Z","shell.execute_reply.started":"2024-07-21T20:25:45.925578Z","shell.execute_reply":"2024-07-21T20:25:45.930910Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# trainning setup..\nEPOCHS=10\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 1e-4\nSCHEDULAR_FACTOR = 0.1\nSCHEDULAR_PATIENCE = 5\nSTOPPING_PATIENCE = 10","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:28:45.651772Z","iopub.execute_input":"2024-07-21T20:28:45.652134Z","iopub.status.idle":"2024-07-21T20:28:45.656883Z","shell.execute_reply.started":"2024-07-21T20:28:45.652106Z","shell.execute_reply":"2024-07-21T20:28:45.655901Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## Trainning","metadata":{}},{"cell_type":"code","source":"# making the model complex according to our GPU resource in kaggle.. \n# for more resource more complex model is recommended..\nEmbedding_Dim = 256\nHidden_Dim = 256\nNum_Layers = 3","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:28:25.211109Z","iopub.execute_input":"2024-07-21T20:28:25.211707Z","iopub.status.idle":"2024-07-21T20:28:25.215895Z","shell.execute_reply.started":"2024-07-21T20:28:25.211676Z","shell.execute_reply":"2024-07-21T20:28:25.214892Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# create a model instance.. \nmodel = My_BiLSTM(word_vocab_total_words, embedding_dim=Embedding_Dim, hidden_dim=Hidden_Dim, num_layers = Num_Layers)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:28:26.514387Z","iopub.execute_input":"2024-07-21T20:28:26.515074Z","iopub.status.idle":"2024-07-21T20:28:27.064460Z","shell.execute_reply.started":"2024-07-21T20:28:26.515041Z","shell.execute_reply":"2024-07-21T20:28:27.063384Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(model, desc, data_loader, k=5):\n    \n    model.load_state_dict(torch.load(CHECKPOINT_PATH))\n    \n    model.eval()\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for batch_x, batch_y in tqdm(data_loader, desc=desc, leave=False):\n            batch_x, batch_y = batch_x.to(device), one_hot(batch_y, num_classes=word_vocab_total_words).to(device)\n            \n            # Forward pass\n            output = model(batch_x)\n\n            # Get top-k predictions\n            _, predicted_indices = output.topk(k, dim=1)\n\n            # Check if the correct label is in the top-k predictions\n            correct_predictions += torch.any(predicted_indices == torch.argmax(batch_y, dim=1, keepdim=True), dim=1).sum().item()\n            total_predictions += batch_y.size(0)\n\n    accuracy = correct_predictions / total_predictions\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:28:27.349360Z","iopub.execute_input":"2024-07-21T20:28:27.349707Z","iopub.status.idle":"2024-07-21T20:28:27.357067Z","shell.execute_reply.started":"2024-07-21T20:28:27.349679Z","shell.execute_reply":"2024-07-21T20:28:27.356116Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:28:29.808286Z","iopub.execute_input":"2024-07-21T20:28:29.808630Z","iopub.status.idle":"2024-07-21T20:28:29.815106Z","shell.execute_reply.started":"2024-07-21T20:28:29.808603Z","shell.execute_reply":"2024-07-21T20:28:29.814289Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"epochs = EPOCHS\nall_accuracies = []\nall_losses = []\nfor epoch in range(epochs):\n    model.train()\n    for batch_X, batch_y in tqdm(train_loader, desc=\"Trainning : \", leave=False):\n        batch_X, batch_y = batch_X.to(device), one_hot(batch_y, num_classes=word_vocab_total_words).to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y.argmax(dim=1))\n        loss.backward()\n        optimizer.step()\n\n    accuracy = calculate_accuracy(model, \"Calculating Accuracy : \", train_loader)\n    print(f'Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}, Train K-Accuracy: {accuracy * 100:.2f}%')\n    all_accuracies.append(accuracy)\n    all_losses.append(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:28:50.035863Z","iopub.execute_input":"2024-07-21T20:28:50.036743Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Trainning :  71%|███████   | 963/1353 [19:59<08:05,  1.25s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing and Results","metadata":{}},{"cell_type":"code","source":"epoch_list = [i for i in range(1,epochs+1)]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n\naxes[0].plot(epoch_list, all_accuracies, color='#5a7da9', label='Accuracy', linewidth=3)\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Accuracy Graph')\naxes[0].grid(True)\n\naxes[1].plot(epoch_list, all_losses, color='#adad3b', label='Accuracy', linewidth=3)\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Loss Graph')\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = calculate_accuracy(model, \"Testing : \", test_loader)\nprint(f'Test-Accuracy: {accuracy * 100:.2f}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving The model","metadata":{}},{"cell_type":"code","source":"model_path = f'./model_biLSTM.pth'\n\ntorch.save(model.state_dict(), model_path)\nprint(f'Model saved to {model_path}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After an intensive training phase, our BiLSTM model has been successfully trained on the rich corpus of Rabindranath Tagore's writings. The model's architecture and training parameters were carefully chosen to handle the intricacies of Bengali language syntax and semantics.\n\n\nThe evaluation metrics indicate that the model performs well in predicting the next word in a given sequence, showcasing its capability in understanding and generating text in Bengali. The loss curve suggests that the model has learned the patterns and nuances of the language effectively over the training epochs.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Demonstration","metadata":{}},{"cell_type":"code","source":"# load the model\nmodel_path = 'bilstm_model.pth'\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:00:47.538790Z","iopub.execute_input":"2024-07-20T18:00:47.539154Z","iopub.status.idle":"2024-07-20T18:00:47.589793Z","shell.execute_reply.started":"2024-07-20T18:00:47.539124Z","shell.execute_reply":"2024-07-20T18:00:47.588922Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Conversion From `Idx-to-Word`","metadata":{}},{"cell_type":"code","source":"def text_to_numerical_sequence_test(tokenized_text):\n    tokens_list = []\n    for token in tokenized_text:\n        num_token = word_to_index[token] if token in word_to_index else word_to_index['<unk>']\n        tokens_list.append(num_token)\n    return tokens_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding Temperature to Predictions\n\n**0.1** -> *for variations in ouput... for creative and experimental answers*\n\n- \n- \n\n**1.0** -> *for accuracy in ouput... for to the point answers*","metadata":{}},{"cell_type":"code","source":"# Define a function to sample from the model's output distribution with temperature\ndef sample_with_temperature(output, temperature=1.0):\n    # Scale the logits by the temperature\n    logits = output / temperature\n    # Convert logits to probabilities\n    probabilities = F.softmax(logits, dim=-1).cpu().detach().numpy()\n    # Sample from the distribution\n    next_word_index = np.random.choice(len(probabilities), p=probabilities)\n    return next_word_index","metadata":{"execution":{"iopub.status.busy":"2024-07-20T22:11:46.071981Z","iopub.execute_input":"2024-07-20T22:11:46.072641Z","iopub.status.idle":"2024-07-20T22:11:46.077767Z","shell.execute_reply.started":"2024-07-20T22:11:46.072609Z","shell.execute_reply":"2024-07-20T22:11:46.076910Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Text Generator","metadata":{}},{"cell_type":"code","source":"# Generate the next 10 words forming a sentence using the RNN model\ndef generate_sentence(ip_sentence, text_model=model, n=10, temperature=0.8):\n    \n    sentence = ip_sentence.split()\n        \n    model.eval()\n    for _ in range(n):\n        tokenized_sequence = text_to_numerical_sequence_test(sentence) \n        \n        padded_X = torch.tensor(F.pad(torch.tensor(tokenized_sequence), (longest_sequence_feature - len(tokenized_sequence),0), value=0)).unsqueeze(0)\n\n        output = model(padded_X)\n\n        next_word_index = output.argmax(dim=1).item()\n        next_word_index = sample_with_temperature(output[0], temperature=temperature)\n        \n        next_word = index_to_word [next_word_index]\n        sentence.append(next_word)\n            \n        tokenized_sequence.append(next_word_index)\n\n        \n    return ' '.join(sentence)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T22:38:22.520356Z","iopub.execute_input":"2024-07-20T22:38:22.520993Z","iopub.status.idle":"2024-07-20T22:38:38.577717Z","shell.execute_reply.started":"2024-07-20T22:38:22.520961Z","shell.execute_reply":"2024-07-20T22:38:38.576809Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3871983314.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  padded_X = torch.tensor(F.pad(torch.tensor(tokenized_sequence), (longest_sequence_feature - len(tokenized_sequence),0), value=0)).unsqueeze(0)\n","output_type":"stream"},{"name":"stdout","text":"\n\n for temp 0.1 -->\n একটি কবিতার খাতা , এই যে সকল মানুষের মধ্যে আমাদের দেশের মধ্যে যে একটা বিশেষ সম্বন্ধ আছে তাহা নহে , তাহা নহে । নহে । তাহার মধ্যে তাহার মধ্যে একটা বিশেষ ঐক্য আছে\n\n\n for temp 0.2 -->\n একটি কবিতার খাতা , এই দুই লোকের মধ্যে যে একটা বিশেষ জিনিস আছে , সেই সকল লোকের মধ্যে যে একটা বিশেষ ঐক্য আছে তাহা নহে , তাহা নহে । নহে । , তাহার\n\n\n for temp 0.3 -->\n একটি কবিতার খাতা , এই যে সকল মানুষের মধ্যে আমাদের দেশের মধ্যে আমাদের দেশের লোকের সঙ্গে যে একটা বিশেষ সম্বন্ধ আছে তাহা নহে , তাহা আমাদের নিজের পক্ষে কোনো বিশেষ কারণ । নহে\n\n\n for temp 0.4 -->\n একটি কবিতার খাতা থেকে একটা বিশেষ ছন্দ , এই কথাটা , তারা তার মধ্যে একটা বৃহৎ রূপ আছে । , আর এক দিকে । মতো , এই সব লোকের মধ্যে তার পরে ।\n\n\n for temp 0.5 -->\n একটি কবিতার খাতা থেকে যে প্রত্যক্ষ আজও তাঁদের সঙ্গে আপন বিরোধ আছে , এই বলে , আমাদের মধ্যে যদি আমাদের কাছে যে সমস্ত পথ দিয়া এমন কথা দেখি তবে আমাদের যে যাহা আমাদের\n\n\n for temp 0.6 -->\n একটি কবিতার খাতা যেখানে কোনো ব্যক্তির সম্বন্ধ খাটাইবার জন্য এ কথা নয় । এত ঘনিষ্ঠ কারণ , আমরা তাঁহার সঙ্গে যে একটি বিশেষ পরিচয় আছে , যে কোনোরূপ লোকের মতো তাহা আমাদের চিত্তকে\n\n\n for temp 0.7 -->\n একটি কবিতার খাতা খুব একটি স্বাভাবিক অপরূপ চরণ , তাঁহার সেই রক্তবীজকেই যে আসক্তি দিয়া দেখা যায় , সেই সকল দিকে বলিতে হইবে । রুচির প্রয়োগ করিলে সৃষ্টিশক্তিকে কোনো পাওয়া যায় না ।\n\n\n for temp 0.8 -->\n একটি কবিতার খাতা বিরাজ করে , সেখানে এত প্রাণের কোনো গ্রাম্য পক্ষে আমাদের বজায় গভীরভাবে শেক্ স্ রে , কিন্তু যখন তা হলে তাঁরা যা করে না , সে রাস্তায় জগতের ভাষা !\n\n\n for temp 0.9 -->\n একটি কবিতার খাতা আছ আমার চিত্তের বিকাশ আছে সে অসহ্য প্রয়োজনের ভাণ্ডার থাকত না , শিক্ষার অবকাশ পাওয়া গেছে । অহমের শ্মশানে । imaginative ও টাকা দিয়েই । প্ল্যান করেছি । । কিছু\n\n\n for temp 1.0 -->\n একটি কবিতার খাতা দিয়াই রঙাইতে শ্রীযুক্ত ট্রা পল্লীসেবা হইলাম সমান এবং জনস্রোতের লীলা ধর্মবুদ্ধিগত ব্যক্ত করিতেছে ওই তো ধুপ্ অথবা কোনো আশ্চর্য বিদ্বেষের কারণ নহে । একখানি হইয়া সমস্তের লোলজিহ্বায় ভাবামৃতের উলকির অথবা\n","output_type":"stream"}]},{"cell_type":"code","source":"## sample inputs\n\n# input_sentence = \"তাঁর একটি কবিতার খাতা\"\n# input_sentence = \"তাঁর একটি\"\n# input_sentence = \"তাঁর কবিতার খাতা\"\ninput_sentence = \"একটি কবিতার খাতা\"\n\n\n\n# input_sentence = \"কিন্তু আমরা\"\n# input_sentence = \"আমরা তো বিজ্ঞানী নই\"\n# input_sentence = \"আমরা তো\"\n# input_sentence = \"বুঝতে পারি\"\n# input_sentence = \"কিন্তু আমরা\"\n# input_sentence = \"কিন্তু আমরা বুঝতে পারি\"\n# input_sentence = \"আমরা তো বিজ্ঞানী \"\n# input_sentence = \"কিন্তু আমরা তো বিজ্ঞানী নই, বুঝতে পারি\"\n\n\n\ntemp= [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\nfor t in temp:\n    sentence = generate_sentence(input_sentence, model, n=30, temperature=t)\n    print(f'\\n\\n\\n for temp {t} -->\\n {sentence}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Future Works","metadata":{}},{"cell_type":"markdown","source":"Moving forward, we plan to:\n\n- **Expand the Dataset**: *Include more works by Tagore and other Bengali authors to enrich the training data.*\n\n- **Experiment with Advanced Architectures**: *Explore transformer-based models and attention mechanisms for potentially better performance.*\n\n- **Deploy the Model**: *Create a user-friendly application for Bengali text generation and sentence completion.*","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we have embarked on an exciting journey to explore Natural Language Processing, specifically focusing on Language Modelling using a Complex BiLSTM model. By utilizing the rich literary works of Rabindranath Tagore, we aimed to predict the next word in a sequence, enhancing the capabilities of text generation and sentence completion in the Bengali language.\n\n\nThrough detailed steps, from data preprocessing to model training and evaluation, we demonstrated how to leverage a significant corpus of around 1 lakh words to train a robust BiLSTM model. This approach not only provides insights into handling Indian languages in NLP tasks but also underscores the importance of utilizing classical literature to enhance modern technological applications.\n\n\nThe successful implementation of this task opens up numerous possibilities for further research and application, such as improving language models for other regional languages, creating more sophisticated text generation systems, and contributing to the preservation and digital enhancement of cultural literary works.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank You","metadata":{}},{"cell_type":"markdown","source":"Thank you for following along with this notebook. We hope it has provided you with a comprehensive and beginner-friendly introduction to NLP tasks, particularly in the context of Indian languages. Whether you are a student, researcher, or enthusiast, we believe that the skills and knowledge gained here will be valuable in your journey into the world of Natural Language Processing.\n\n\nWe would like to express our gratitude to the timeless works of Rabindranath Tagore, which provided the foundation for this project. Their linguistic richness and cultural significance have greatly contributed to the success of our model.\n\n\nHappy learning and happy coding!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainning","metadata":{}},{"cell_type":"code","source":"# Function for Loading the Previously Saved State Dictionary of the Model.. if available\ndef load_model(model, name, model_path):\n    if os.path.exists(model_path):\n      print(f\"\\nLoading Saved Version of the Model {name} ....\\n\")\n      model.load_state_dict(torch.load(model_path)) \n    \n    else :\n      print(f\"\\nFOUND NO Previous Saved Version of the Model {name} ....\\n\")\n\n    return model ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trainer..\n\ndef trainer(model, name, check_path, train_loader, valid_loader, epochs = 500, learning_rate = 0.001):\n    \n    CHECKPOINT_PATH = check_path\n    name = name\n    \n    model = load_model(model, name, CHECKPOINT_PATH ).to(device)\n\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n#     optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=WEIGHT_DECAY)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULAR_FACTOR, patience=SCHEDULAR_PATIENCE, verbose=True)\n\n    # Define the early stopping criteria\n    best_val_loss = float('inf')\n    best_val_acc = 0.0\n    best_train_loss = float('inf')\n    best_train_acc = 0.0\n    patience = STOPPING_PATIENCE \n    counter = 0\n\n    # Lists to store accuracy and loss values\n    train_acc_list = []\n    train_loss_list = []\n    val_acc_list = []\n    val_loss_list = []\n\n    print(f\"\\nStarting Trainning... for model {name}\\n\\n\")\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n\n        for inputs, labels in tqdm(train_loader, desc=\"Trainning : \", leave=False):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            train_correct += (predicted == labels).sum().item()\n\n        train_loss /= len(train_loader.dataset)\n        train_acc = train_correct / len(train_loader.dataset)\n        train_acc_list.append(train_acc)\n        train_loss_list.append(train_loss)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n\n        with torch.no_grad():\n            for inputs, labels in tqdm(valid_loader, desc=\"Validating : \", leave=False):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs.data, 1)\n                val_correct += (predicted == labels).sum().item()\n\n            val_loss /= len(valid_loader.dataset)\n            val_acc = val_correct / len(valid_loader.dataset)\n            val_acc_list.append(val_acc)\n            val_loss_list.append(val_loss)\n            \n        # Step the scheduler\n        scheduler.step(val_loss)\n\n        print(f\"Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n\n        \n        # Early stopping based on validation loss\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            counter = 0\n        else:\n            counter += 1\n            if counter >= patience:\n                print(\"Early stopping!\")\n                break\n                \n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            best_train_acc = train_acc\n            best_train_loss = train_loss\n            \n            # Save the model if it has the best validation accuracy\n            torch.save(model.state_dict(), CHECKPOINT_PATH)\n\n    print(f\"\\n\\n\\nBEST MODEL --> \\nTrain Acc : {best_train_acc:.4f} | Train Loss : {best_train_loss:.4f} | Valid Acc : {best_val_acc:.4f} | Valid Loss : {best_val_loss:.4f}\")\n\n    # Plot accuracy and loss curves\n    plt.figure(figsize=(8, 4))\n    plt.plot(train_acc_list, label='Train')\n    plt.plot(val_acc_list, label='Validation')\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(loc='upper left')\n    plt.savefig(f\"./Acc_{name}.png\")\n    plt.show()\n\n\n    plt.figure(figsize=(8, 4))\n    plt.plot(train_loss_list, label='Train')\n    plt.plot(val_loss_list, label='Validation')\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(loc='upper left')\n    plt.savefig(f\"./loss_{name}.png\")\n    plt.show()\n    \n    return model, [best_train_loss, best_train_acc, best_val_loss, best_val_acc]","metadata":{"execution":{"iopub.status.busy":"2024-07-20T19:33:45.598447Z","iopub.execute_input":"2024-07-20T19:33:45.598767Z","iopub.status.idle":"2024-07-20T22:03:10.916579Z","shell.execute_reply.started":"2024-07-20T19:33:45.598733Z","shell.execute_reply":"2024-07-20T22:03:10.915335Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\nFOUND NO Previous Saved Version of the Model biLSTM_model ....\n\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/100 | Train Loss: 7.9871 | Train Acc: 0.0711 | Val Loss: 7.6775 | Val Acc: 0.0815\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 2/100 | Train Loss: 7.5820 | Train Acc: 0.0864 | Val Loss: 7.3832 | Val Acc: 0.0965\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 3/100 | Train Loss: 7.3325 | Train Acc: 0.0978 | Val Loss: 7.2311 | Val Acc: 0.1005\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 4/100 | Train Loss: 7.2011 | Train Acc: 0.1044 | Val Loss: 7.1442 | Val Acc: 0.1104\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 5/100 | Train Loss: 7.1003 | Train Acc: 0.1092 | Val Loss: 7.0914 | Val Acc: 0.1131\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 198\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m#     log_output(CLASSES, best_train_loss, best_train_acc, best_val_loss, best_val_acc, test_loss,test_acc,average_acc,top2_acc,top5_acc,each_acc, LEARNING_RATE, epoch, EPOCHS, f\"./{dataset['folder']}/{model_dict['folder']}/results_{name}.txt\")\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, [best_train_loss, best_train_acc, best_val_loss, best_val_acc, test_loss, test_acc]\n\u001b[0;32m--> 198\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 72\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(model, train_loader, valid_loader, test_loader, EPOCHS, batch_size, LEARNING_RATE, measure_performance)\u001b[0m\n\u001b[1;32m     70\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     75\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"name = \"biLSTM\"\ncheck_path = f'./model_{name}.pth'\n\n# trainning... the model\nmodel, train_results = trainer(model, name, chech_path, train_loader, valid_loader, EPOCHS=epochs, learning_rate=LEARNING_RATE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def tester(model, CHECKPOINT_PATH, test_loader):\n    # Load the best model checkpoint for evaluation\n    model.load_state_dict(torch.load(CHECKPOINT_PATH))\n\n    # Evaluate on the test set\n    model.eval()\n    test_loss = 0.0\n    test_correct = 0\n    top2_correct=0\n    top5_correct=0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Testing : \", leave=False):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            test_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            _,top2preds = torch.topk(outputs.data,2,1)\n            _,top5preds = torch.topk(outputs.data,5,1)\n            test_correct += (predicted == labels).sum().item()\n            for i,label in enumerate(labels):\n              if label in top5preds[i]:\n                top5_correct += 1\n              if label in top2preds[i]:\n                top2_correct += 1\n\n        test_loss /= len(test_loader.dataset)\n        test_acc = test_correct / len(test_loader.dataset)\n        top2_acc = top2_correct/len(test_loader.dataset)\n        top5_acc = top5_correct/len(test_loader.dataset)\n        \n        \n    print(f\"Test Loss : {test_loss:.4f} | Test Acc: {test_acc:.4f} | Top-2 Acc: {top2_acc:.4f} | Top-5 Acc: {top5_acc:.4f}\")\n\n    return [test_loss, test_acc, top2_acc, top5_acc]\n\n\n# trainning... the model\ntest_results = tester(model, chech_path, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-20T22:11:45.381532Z","iopub.execute_input":"2024-07-20T22:11:45.381861Z","iopub.status.idle":"2024-07-20T22:11:45.386921Z","shell.execute_reply.started":"2024-07-20T22:11:45.381836Z","shell.execute_reply":"2024-07-20T22:11:45.385976Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}