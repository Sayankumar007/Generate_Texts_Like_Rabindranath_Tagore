{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004315,"sourceType":"datasetVersion","datasetId":1784343}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Preprocessing Techniques On Bengali Text using Advanced Text Preprocessing Tools with Efficient Memory Utilization and Step-by-Step Guide for Beginners\n\n#### Text Preprocessing is very Important Task for the Efficiency of Language Models, in NLP. Beginners are often Confused used with the Text datasets, specially when its comes to Indian Languages, due to their complexity and huge vocabulary. \n\n#### *In this Notebook, I have Explained all necessary text preprocessing techniques using most advanced NLP tools with step-by-step guide for beginners. I have also showed some techniques for Efficient Memory Utilization when working with big datasets and making the dataset ready for Trainning a Language Model.*\n\n#### Here as a DATASET, I have used Text Corpus from the Essays of Great Bengali Writer, Poet, Philosopher [Rabindranath Tagore](http://en.wikipedia.org/wiki/Rabindranath_Tagore). These are one of the best Scriptures ever you found in Bengali Language. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when we create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-07T21:04:43.299770Z","iopub.execute_input":"2024-03-07T21:04:43.300092Z","iopub.status.idle":"2024-03-07T21:04:44.278299Z","shell.execute_reply.started":"2024-03-07T21:04:43.300061Z","shell.execute_reply":"2024-03-07T21:04:44.277377Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/csv/essay.csv\n/kaggle/input/csv/drama.csv\n/kaggle/input/csv/all_collection.csv\n/kaggle/input/csv/poem.csv\n/kaggle/input/csv/misc.csv\n/kaggle/input/csv/novel.csv\n/kaggle/input/csv/story.csv\n/kaggle/input/csv/song.csv\n/kaggle/input/txt/essay.txt\n/kaggle/input/txt/novel.txt\n/kaggle/input/txt/song.txt\n/kaggle/input/txt/poem.txt\n/kaggle/input/txt/story.txt\n/kaggle/input/txt/drama.txt\n/kaggle/input/txt/misc.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Getting The Dataset","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/input/txt/essay.txt\"\n\nwith open(file_path, 'r') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:04:44.280165Z","iopub.execute_input":"2024-03-07T21:04:44.280760Z","iopub.status.idle":"2024-03-07T21:04:45.008891Z","shell.execute_reply.started":"2024-03-07T21:04:44.280726Z","shell.execute_reply":"2024-03-07T21:04:45.007842Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"text[0:200]","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:04:45.017352Z","iopub.execute_input":"2024-03-07T21:04:45.018275Z","iopub.status.idle":"2024-03-07T21:04:45.025493Z","shell.execute_reply.started":"2024-03-07T21:04:45.018241Z","shell.execute_reply":"2024-03-07T21:04:45.024663Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'আশ্রমের রূপ ও বিকাশ ২\\nশিলাইদহে পদ্মাতীরে সাহিত্যচর্চা নিয়ে নিভৃতে বাস করতুম। একটা সৃষ্টির সংকল্প নিয়ে সেখান থেকে এলেম শান্তিনিকেতনের প্রান্তরে।\\nতখন আশ্রমের পরিধি ছিল ছোটো। তার দক্ষিণ সীমানায় দীর্ঘ সার'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Remove Expressions (WhiteSpaces and Punctuations)","metadata":{}},{"cell_type":"markdown","source":"#### \"\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a\": This part of the pattern specifies a character class that matches different types of whitespace characters.\n\n**\\s** --> Matches any whitespace character, which includes spaces, tabs, and newline characters\n\n**\\u0020** --> Unicode character U+0020, which represents a space character.\n\n**\\u00a0** --> Unicode character U+00A0, which represents a non-breaking space (NBSP).\n\n**\\u1680, \\u180e, \\u202f, \\u205f, \\u3000** --> Various Unicode characters representing different types of space characters.\n\n**\\u2000-\\u200a** --> A range of Unicode characters representing different types of space characters.\n\n*re.UNICODE* --> *This flag is used to indicate that the pattern and the input text should be treated as Unicode strings, enabling the use of Unicode character escapes like \\u0020, \\u00a0, etc.*","metadata":{}},{"cell_type":"code","source":"import re  # built-in python library for recognizing regular expressions and patterns\n\n# replacing all punctuation symbols and whitespaces with a single whitepace\n\nwhitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\nbangla_fullstop = u\"\\u0964\"  # unicode code for bengali fullstop\npunctSeq = u\"['\\\"“”‘’]+|[.…-]+|[:;]+\"\nbengali_numeral_pattern = r'[১-৯০]+' # Regular expression pattern to match Bengali numerals\n\n\n\ncorpus = re.sub(bengali_numeral_pattern,\" \",text)   # Remove individual Bengali numerals..\ncorpus = re.sub(r'\\s*\\d+\\s*', ' ', corpus)          # Remove all possible sequences of Bengali numerals..\n\ncorpus = re.sub('\\n',\" \",corpus)                                # remove all '\\n' symbols\ncorpus = re.sub(punctSeq,\" \",corpus)                            # remove all punctuations..\ncorpus = whitespace.sub(\" \",corpus).strip()                     # the strip() method to remove any leading or trailing whitespace from the resulting string \n\n\n\n\n# we are skipping the punctuations like (? ! ,) etc. as they are very important in Bengali Sentences.. \n# we can use the below variables to remove those for other task like classification etc. if we want. \n\n\n# bangla_fullstop = u\"\\u0964\"  # unicode code for bengali fullstop\n# punctSeq = u\"['\\\"“”‘’]+|[.?!,…-]+|[:;]+\"\n\n# corpus = re.sub(bangla_fullstop,\"\",corpus)      # removing bengali fullstop fully from the corpus","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:09.277672Z","iopub.execute_input":"2024-03-07T21:05:09.278295Z","iopub.status.idle":"2024-03-07T21:05:13.707301Z","shell.execute_reply.started":"2024-03-07T21:05:09.278264Z","shell.execute_reply":"2024-03-07T21:05:13.706233Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"corpus[0:500]","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:13.709053Z","iopub.execute_input":"2024-03-07T21:05:13.709454Z","iopub.status.idle":"2024-03-07T21:05:13.721777Z","shell.execute_reply.started":"2024-03-07T21:05:13.709406Z","shell.execute_reply":"2024-03-07T21:05:13.720831Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'আশ্রমের রূপ ও বিকাশ শিলাইদহে পদ্মাতীরে সাহিত্যচর্চা নিয়ে নিভৃতে বাস করতুম। একটা সৃষ্টির সংকল্প নিয়ে সেখান থেকে এলেম শান্তিনিকেতনের প্রান্তরে। তখন আশ্রমের পরিধি ছিল ছোটো। তার দক্ষিণ সীমানায় দীর্ঘ সার বাঁধা শালগাছ। মাধবীলতা বিতানে প্রবেশের দ্বার। পিছনে পুব দিকে আমবাগান, পশ্চিম দিকে কোথাও বা তাল, কোথাও বা জাম, কোথাও বা ঝাউ, ইতস্তত গুটিকয়েক নারকেল। উত্তরপশ্চিম প্রান্তে প্রাচীন দুটি ছাতিমের তলায় মার্বেল পাথরে বাঁধানো একটি নিরলংকৃত বেদী। তার সামনে গাছের আড়াল নেই, দিগন্ত পর্যন্ত অবারিত মাঠ, সে মাঠে তখন'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Word Tokenization: Splitting the Sentences into Word Tokens","metadata":{}},{"cell_type":"markdown","source":"#### I am using NLTK's WordTokenizer for this task, but you can explore other tokenizers [here](https://www.nltk.org/api/nltk.tokenize.html)","metadata":{}},{"cell_type":"code","source":"# Tokenization can be done just by using .split() function if we remove all of the punctuation symbols\n# we just have to use below code\n\n# corpus.split()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:17.754930Z","iopub.execute_input":"2024-03-07T21:05:17.755734Z","iopub.status.idle":"2024-03-07T21:05:17.759240Z","shell.execute_reply.started":"2024-03-07T21:05:17.755704Z","shell.execute_reply":"2024-03-07T21:05:17.758371Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# In our case, some symbols aren't removed for better sentence formation.. so, it is better to go with\n# an optimized library function, I am using NLTK here but there are also keras, torchtext, spacy etc. librarie\nfrom nltk.tokenize import word_tokenize\n\n\n\n# Tokenize sentences based on the Bengali fullstop symbol\nsentences = corpus.split(bangla_fullstop)\n\n# Remove empty strings and add the period symbol back to each sentence\nsentences = [sentence.strip() + \" \" + bangla_fullstop for sentence in sentences if sentence.strip()]\n\n# Tokenize each sentence into words\ndoc = [word_tokenize(sentence) for sentence in sentences]","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:18.379131Z","iopub.execute_input":"2024-03-07T21:05:18.379495Z","iopub.status.idle":"2024-03-07T21:05:43.264649Z","shell.execute_reply.started":"2024-03-07T21:05:18.379464Z","shell.execute_reply":"2024-03-07T21:05:43.263790Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### **This Sentence Tokenization is very very necessary for a raw corpus. Just Word Tokenization will not help Word Embeddings In Getting The proper Context of the sentence. Try and see results using word Enbedding only**","metadata":{}},{"cell_type":"code","source":"print(\"Before Tokenization : \", corpus[-386:],\"\\n\")\nprint(\"After Tokenization : \", doc[-3:])","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:43.266159Z","iopub.execute_input":"2024-03-07T21:05:43.266586Z","iopub.status.idle":"2024-03-07T21:05:43.272425Z","shell.execute_reply.started":"2024-03-07T21:05:43.266558Z","shell.execute_reply":"2024-03-07T21:05:43.271386Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Before Tokenization :  কিন্তু আমরা তো বিজ্ঞানী নই, বুঝতে পারি নে হঠাৎ অঙ্কের আরম্ভ হয় কোথা থেকে, একেবারে শেষই বা হয় কোন্ খানে। সম্পূর্ণ সংঘটিত বিশ্বকে নিয়ে হঠাৎ কালের আরম্ভ হল আর সদ্যোলুপ্ত বিশ্বের সঙ্গে কালের সম্পূর্ণ অন্ত হবে, আমাদের বুদ্ধিতে এর কিনারা পাই নে। বিজ্ঞানী বলবেন, বুদ্ধির কথা এখানে আসছে না, এ হল গণনার কথা সে গণনা বর্তমান ঘটনাধারার উপরে প্রতিষ্ঠিত এর আদি অন্তে যদি অন্ধকার দেখি তা হলে উপায় নেই। \n\nAfter Tokenization :  [['কিন্তু', 'আমরা', 'তো', 'বিজ্ঞানী', 'নই', ',', 'বুঝতে', 'পারি', 'নে', 'হঠাৎ', 'অঙ্কের', 'আরম্ভ', 'হয়', 'কোথা', 'থেকে', ',', 'একেবারে', 'শেষই', 'বা', 'হয়', 'কোন্', 'খানে', '।'], ['সম্পূর্ণ', 'সংঘটিত', 'বিশ্বকে', 'নিয়ে', 'হঠাৎ', 'কালের', 'আরম্ভ', 'হল', 'আর', 'সদ্যোলুপ্ত', 'বিশ্বের', 'সঙ্গে', 'কালের', 'সম্পূর্ণ', 'অন্ত', 'হবে', ',', 'আমাদের', 'বুদ্ধিতে', 'এর', 'কিনারা', 'পাই', 'নে', '।'], ['বিজ্ঞানী', 'বলবেন', ',', 'বুদ্ধির', 'কথা', 'এখানে', 'আসছে', 'না', ',', 'এ', 'হল', 'গণনার', 'কথা', 'সে', 'গণনা', 'বর্তমান', 'ঘটনাধারার', 'উপরে', 'প্রতিষ্ঠিত', 'এর', 'আদি', 'অন্তে', 'যদি', 'অন্ধকার', 'দেখি', 'তা', 'হলে', 'উপায়', 'নেই', '।']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Word Embedding (Representing Words as Vectors)","metadata":{}},{"cell_type":"markdown","source":"#### In this step, I am using [Word2Vec](https://en.wikipedia.org/wiki/Word2vec#:~:text=Word2vec%20is%20a%20technique%20in,text%20in%20a%20large%20corpus.) Model from Gensim Library by Google and it is one of the best Vectorizer.\n#### We can also use OpenAI's Glove, Facebook's FastText etc. popular Word Embedding methods. Pre-trainedLLM based Embeddings can also be used for more better results..","metadata":{}},{"cell_type":"markdown","source":"#### The Parameters of Word2Vec:\n* **min_count** --> *int - Ignores all words with total absolute frequency loour than this - (2, 100)*\n* **window** --> *int - The maximum distance between the current and predicted word within a sentence. E.g.* *window words on the left and window words on the left of our target - (2, 10)*\n* **vector_size** --> *int - Dimensionality of the feature vectors. - (50, 300)*\n* **sample** --> *float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)*\n* **alpha** --> *float - The initial learning rate - (0.01, 0.05)*\n* **min_alpha** --> *float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00*\n* **negative** --> *int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)*\n* **workers** --> *int - Use these many worker threads to train the model (=faster training with multicore machines)*\n* **sg** --> *bool - Switch between mode - 0 for CBOW and 1 for Skip-gram","metadata":{}},{"cell_type":"code","source":"from gensim.models.word2vec import Word2Vec   # gensim is Google's Text Library\n\ndata = doc \n\n# Initializing the model..\nmodel =  Word2Vec(window=5, min_count=1, epochs=50, workers=3, sg=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:43.273486Z","iopub.execute_input":"2024-03-07T21:05:43.273745Z","iopub.status.idle":"2024-03-07T21:05:53.635944Z","shell.execute_reply.started":"2024-03-07T21:05:43.273723Z","shell.execute_reply":"2024-03-07T21:05:53.634972Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# building the vocabulary..\nmodel.build_vocab(data, progress_per=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:53.637962Z","iopub.execute_input":"2024-03-07T21:05:53.638406Z","iopub.status.idle":"2024-03-07T21:05:59.346907Z","shell.execute_reply.started":"2024-03-07T21:05:53.638379Z","shell.execute_reply":"2024-03-07T21:05:59.345941Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# no. of sentences in the corpus.. in our case we have one sentence as we are considering whole text..\nmodel.corpus_count","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:59.348106Z","iopub.execute_input":"2024-03-07T21:05:59.348471Z","iopub.status.idle":"2024-03-07T21:05:59.354714Z","shell.execute_reply.started":"2024-03-07T21:05:59.348437Z","shell.execute_reply":"2024-03-07T21:05:59.353855Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"98304"},"metadata":{}}]},{"cell_type":"code","source":"# Trainning the model\nmodel.train(data, total_examples=model.corpus_count, epochs=model.epochs)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:05:59.355774Z","iopub.execute_input":"2024-03-07T21:05:59.356009Z","iopub.status.idle":"2024-03-07T21:07:27.147037Z","shell.execute_reply.started":"2024-03-07T21:05:59.355988Z","shell.execute_reply":"2024-03-07T21:07:27.146166Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(77089057, 90311050)"},"metadata":{}}]},{"cell_type":"markdown","source":"**we have done this in 3 steps :**\n*                                      -> Initializing the Model\n*                                      -> Creating the vocabulary\n*                                      -> Training the model with our data\n\n\n*but we can directly use this code :* ","metadata":{}},{"cell_type":"code","source":"# model =  word2vec.Word2Vec(data, window=5, min_count=1, epochs=50, workers=3)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:17.734721Z","iopub.execute_input":"2024-03-07T21:09:17.735371Z","iopub.status.idle":"2024-03-07T21:09:17.739223Z","shell.execute_reply.started":"2024-03-07T21:09:17.735336Z","shell.execute_reply":"2024-03-07T21:09:17.738307Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Saving and Loading The Model.","metadata":{}},{"cell_type":"code","source":"model.save(\"word2vec.model\")  # save the model on the current vocabulary for future use\n\n# model_loaded =  Word2Vec(window=5, min_count=1, epochs=50, workers=3, sg=0)\n# model_loaded = model_loaded.load(\"word2vec.model\")  # load the model for further training and results","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:18.862587Z","iopub.execute_input":"2024-03-07T21:09:18.862924Z","iopub.status.idle":"2024-03-07T21:09:18.994576Z","shell.execute_reply.started":"2024-03-07T21:09:18.862897Z","shell.execute_reply":"2024-03-07T21:09:18.993795Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### **Efficient way to SAVE WORD2VEC Models**\n\n#### The reason for separating the trained vectors into KeyedVectors is that if we don’t need the full model state any more (don’t need to continue training), its state can be discarded, keeping just the vectors and their keys proper.\n\n#### This results in a much smaller and faster object that can be mmapped for lightning fast loading and sharing the vectors in RAM between processes:","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n# Store just the words + their trained embeddings.\nword_vectors = model.wv\nword_vectors.save(\"word2vec.wordvectors\")\n\n# Load back with memory-mapping = read-only, shared across processes.\nwv_model = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n\n# vector = wv['যদি']  # Get numpy vector of a word","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:21.235982Z","iopub.execute_input":"2024-03-07T21:09:21.236637Z","iopub.status.idle":"2024-03-07T21:09:21.386438Z","shell.execute_reply.started":"2024-03-07T21:09:21.236602Z","shell.execute_reply":"2024-03-07T21:09:21.385463Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# checking the vector.. for a word..\nwv_model[\"রূপ\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:23.085858Z","iopub.execute_input":"2024-03-07T21:09:23.086205Z","iopub.status.idle":"2024-03-07T21:09:23.093491Z","shell.execute_reply.started":"2024-03-07T21:09:23.086177Z","shell.execute_reply":"2024-03-07T21:09:23.092626Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"memmap([ 2.1715553 ,  4.9525957 ,  1.3760954 ,  1.7535006 ,  2.6126792 ,\n        -2.0280383 , -0.7900795 , -0.99939203, -0.82516396,  2.953906  ,\n        -0.42472816, -1.4176532 , -0.09955337,  0.33950245,  0.49991438,\n         0.9455731 , -3.06372   , -0.7914227 , -0.13997027,  0.37106255,\n         0.1149399 ,  0.702935  ,  3.489333  ,  0.6260979 ,  0.24333659,\n        -0.270482  ,  3.137865  ,  0.43042314,  1.213598  ,  1.4718145 ,\n        -2.4773455 , -3.6729043 ,  0.35408932,  2.5527062 , -1.7275608 ,\n        -0.5443683 , -2.085737  , -3.864463  , -1.2617679 ,  0.64541453,\n         2.5411851 , -4.3118243 , -1.986154  , -2.326378  , -0.9940671 ,\n         3.3657749 , -1.988101  , -0.8467613 ,  2.1447303 , -0.5635085 ,\n         0.08755372, -2.3738945 ,  0.6189976 , -0.21136978, -0.20596905,\n        -1.038937  , -3.3289661 , -1.9228063 ,  1.1137915 ,  0.01329757,\n         1.9852916 ,  1.9290367 , -0.46294188, -1.1121498 , -0.3246893 ,\n        -4.0741644 ,  1.4964786 , -2.7445173 , -0.49213734, -3.3222222 ,\n         0.57420695,  1.586763  , -1.475476  , -0.7639621 ,  0.82959694,\n         0.94897264, -0.09489205, -2.4583073 ,  1.8199106 , -0.7150969 ,\n         0.31371483, -0.37418774,  1.9507893 ,  0.18179876,  0.41238245,\n         1.3892987 ,  2.5616956 , -1.6678752 , -1.2330728 ,  0.08998661,\n        -1.2752175 , -2.439386  ,  0.86779094,  0.02823044, -3.4539163 ,\n        -0.6912416 , -1.8980776 , -1.502443  ,  1.5055817 ,  3.5751293 ],\n       dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# now check the model efficiency.. by generating similar words of a given word..\n\nwv_model.most_similar(\"রূপ\", topn=10)  # get other top 10 similar words","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:23.777083Z","iopub.execute_input":"2024-03-07T21:09:23.777859Z","iopub.status.idle":"2024-03-07T21:09:23.819267Z","shell.execute_reply.started":"2024-03-07T21:09:23.777827Z","shell.execute_reply":"2024-03-07T21:09:23.817955Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[('মূর্তি', 0.5897358059883118),\n ('আকার', 0.5745710730552673),\n ('রস', 0.5638467073440552),\n ('কাঠামো', 0.5543402433395386),\n ('রূপকে', 0.5288754105567932),\n ('সুসংহত', 0.4986017048358917),\n ('উপাদান', 0.4917798936367035),\n ('চিত্র', 0.48887768387794495),\n ('ভাষা', 0.48708978295326233),\n ('মহিমা', 0.4791184365749359)]"},"metadata":{}}]},{"cell_type":"code","source":"wv_model.most_similar('যদি', topn=5)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:25.171084Z","iopub.execute_input":"2024-03-07T21:09:25.171406Z","iopub.status.idle":"2024-03-07T21:09:25.190619Z","shell.execute_reply.started":"2024-03-07T21:09:25.171382Z","shell.execute_reply":"2024-03-07T21:09:25.189443Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[('তবে', 0.45079946517944336),\n ('না', 0.446536123752594),\n ('পাছে', 0.42494985461235046),\n ('অভিযুক্তই', 0.4117412567138672),\n ('কিনা', 0.4054877460002899)]"},"metadata":{}}]},{"cell_type":"code","source":"wv_model.most_similar('উপায়', topn=5)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:26.293148Z","iopub.execute_input":"2024-03-07T21:09:26.293860Z","iopub.status.idle":"2024-03-07T21:09:26.306363Z","shell.execute_reply.started":"2024-03-07T21:09:26.293826Z","shell.execute_reply":"2024-03-07T21:09:26.304884Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[('পন্থা', 0.7203671932220459),\n ('জো', 0.5813577175140381),\n ('সদুপায়', 0.5678456425666809),\n ('সুবিচার', 0.5632282495498657),\n ('সুযোগ', 0.5539038181304932)]"},"metadata":{}}]},{"cell_type":"code","source":"# Checking similarity between 2 given words...\n\nwv_model.similarity(\"শেষ\", \"অবসান\")","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:27.285969Z","iopub.execute_input":"2024-03-07T21:09:27.286317Z","iopub.status.idle":"2024-03-07T21:09:27.293082Z","shell.execute_reply.started":"2024-03-07T21:09:27.286287Z","shell.execute_reply":"2024-03-07T21:09:27.292134Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0.56753045"},"metadata":{}}]},{"cell_type":"code","source":"wv_model.similarity(\"শেষ\", \"সমাপ্ত\")","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:28.435080Z","iopub.execute_input":"2024-03-07T21:09:28.436055Z","iopub.status.idle":"2024-03-07T21:09:28.441955Z","shell.execute_reply.started":"2024-03-07T21:09:28.436021Z","shell.execute_reply":"2024-03-07T21:09:28.441169Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0.51476955"},"metadata":{}}]},{"cell_type":"markdown","source":"### If you know Bengali then You will surely become so much impressed with the accuracy of our word2vec model","metadata":{}},{"cell_type":"markdown","source":"# Sequence to N-Grams\n\n### Why This Is Needed ?\n--> Sequence to N-gram processing, a crucial step for text generation because, In machine learning, our goal is to predict outcomes based on the provided data. For this task, we aim to predict the next word given the first few words of a sentence. Thus, we need to prepare our data accordingly.**\n\nWhat we'll do here is convert each sentence into an n-gram format. This means breaking it down into sequences, with the last word as the target.\n\n### For Example:\n#### n_grams of Variable Length need Padding\n**Source Data :** *I Am Learning Artificial Intelligence*\n* **X---------------------------------y**\n* *I-----------------------------------Am*\n* *I Am------------------------------Learning*\n* *I Am Learning--------------------Artificial*\n* *I Am Learning Artificial----------Intelligence*\n\n\n#### n_grams of Fixed Length\n**Source Data :** *I Am Interested in Learning Artificial Intelligence*\n* **X---------------------------------y**\n* *I Am Interested ------------------In*\n* *Am Interested In-----------------Learning*\n* *Interested In Learning -----------Artificial*\n* *In Learning Artificial-------------Intelligence*\n\n\n**In essence, we start with the first word of a sentence as the feature, and the next word becomes the label. We repeat this process until the end. So, now our model has a dataset that tells it, given a sequence, what the next word should be.**\n\n**Later, we'll add padding to ensure that each feature-label pair has the same size for variable n-grams.**","metadata":{}},{"cell_type":"code","source":"# function for n-grams of variable length:\n\n# def seq2grams(sentences, vector):\n#     n_grams = []\n#     for sentence in sentences:            # for each sentence in the corpus\n#         words = vector[sentence]\n#         for i in range(1, len(words)):  # from [1st] word, [1st,2nd] word, [1st,2nd,3rd] word upto last word inde \n#             sequence = words[:i+1]      # make sequences [1,2], [1,2,3], [1,2,3,4] and so on\n#             n_grams.append(sequence)        # add the sequence to the main array\n#     return n_grams\n\n# data_set = seq2grams(data, model.wv)\n# print(train_data[0])    # will have 2 word vectors\n# print(train_data[1])    # will have 3 word vectors","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:30.259142Z","iopub.execute_input":"2024-03-07T21:09:30.259475Z","iopub.status.idle":"2024-03-07T21:09:30.264123Z","shell.execute_reply.started":"2024-03-07T21:09:30.259449Z","shell.execute_reply":"2024-03-07T21:09:30.263075Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# function for n-grams of fixed n-length:\n\nn = 10\ndef seq2grams(sentences, vector):\n    n_grams = []\n    for sentence in sentences:            # for each sentence in the corpus\n        words = vector[sentence]\n        for i in range(1, len(words)-n+1):  # iterate all word upto last word index \n            sequence = words[i:i+n]         # make sequences [1,2,3,4], [2,3,4,5], [3,4,5,6] and so on\n            n_grams.append(sequence)        # add the sequence to the main array\n    return n_grams\n\n\ndata_set = seq2grams(data, wv_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:30.593950Z","iopub.execute_input":"2024-03-07T21:09:30.594507Z","iopub.status.idle":"2024-03-07T21:09:47.171470Z","shell.execute_reply.started":"2024-03-07T21:09:30.594481Z","shell.execute_reply":"2024-03-07T21:09:47.170651Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(data_set)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:47.173224Z","iopub.execute_input":"2024-03-07T21:09:47.173586Z","iopub.status.idle":"2024-03-07T21:09:47.179800Z","shell.execute_reply.started":"2024-03-07T21:09:47.173549Z","shell.execute_reply":"2024-03-07T21:09:47.178937Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"886850"},"metadata":{}}]},{"cell_type":"code","source":"data_set[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:09:47.180754Z","iopub.execute_input":"2024-03-07T21:09:47.180999Z","iopub.status.idle":"2024-03-07T21:09:47.203026Z","shell.execute_reply.started":"2024-03-07T21:09:47.180977Z","shell.execute_reply":"2024-03-07T21:09:47.202176Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"array([[ 2.17155528e+00,  4.95259571e+00,  1.37609541e+00,\n         1.75350058e+00,  2.61267924e+00, -2.02803826e+00,\n        -7.90079474e-01, -9.99392033e-01, -8.25163960e-01,\n         2.95390606e+00, -4.24728155e-01, -1.41765320e+00,\n        -9.95533690e-02,  3.39502454e-01,  4.99914378e-01,\n         9.45573092e-01, -3.06371999e+00, -7.91422725e-01,\n        -1.39970273e-01,  3.71062547e-01,  1.14939898e-01,\n         7.02934980e-01,  3.48933291e+00,  6.26097918e-01,\n         2.43336588e-01, -2.70482004e-01,  3.13786507e+00,\n         4.30423141e-01,  1.21359801e+00,  1.47181451e+00,\n        -2.47734547e+00, -3.67290425e+00,  3.54089320e-01,\n         2.55270624e+00, -1.72756076e+00, -5.44368327e-01,\n        -2.08573699e+00, -3.86446309e+00, -1.26176786e+00,\n         6.45414531e-01,  2.54118514e+00, -4.31182432e+00,\n        -1.98615396e+00, -2.32637811e+00, -9.94067073e-01,\n         3.36577487e+00, -1.98810101e+00, -8.46761286e-01,\n         2.14473033e+00, -5.63508511e-01,  8.75537172e-02,\n        -2.37389445e+00,  6.18997574e-01, -2.11369783e-01,\n        -2.05969051e-01, -1.03893697e+00, -3.32896614e+00,\n        -1.92280626e+00,  1.11379147e+00,  1.32975671e-02,\n         1.98529160e+00,  1.92903674e+00, -4.62941885e-01,\n        -1.11214983e+00, -3.24689299e-01, -4.07416439e+00,\n         1.49647856e+00, -2.74451733e+00, -4.92137343e-01,\n        -3.32222223e+00,  5.74206948e-01,  1.58676302e+00,\n        -1.47547603e+00, -7.63962090e-01,  8.29596937e-01,\n         9.48972642e-01, -9.48920548e-02, -2.45830727e+00,\n         1.81991065e+00, -7.15096891e-01,  3.13714832e-01,\n        -3.74187738e-01,  1.95078933e+00,  1.81798756e-01,\n         4.12382454e-01,  1.38929868e+00,  2.56169558e+00,\n        -1.66787517e+00, -1.23307276e+00,  8.99866149e-02,\n        -1.27521753e+00, -2.43938589e+00,  8.67790937e-01,\n         2.82304361e-02, -3.45391631e+00, -6.91241622e-01,\n        -1.89807761e+00, -1.50244296e+00,  1.50558174e+00,\n         3.57512927e+00],\n       [ 8.19047332e-01, -2.54673076e+00,  4.20967674e+00,\n        -2.54114151e+00,  6.36473060e-01, -2.29021692e+00,\n         1.39741230e+00, -1.29700196e+00,  2.47246727e-01,\n         2.20250773e+00,  1.42712057e+00, -1.07749045e+00,\n         1.81653470e-01, -1.89780325e-01, -2.33429402e-01,\n         2.24184227e+00, -4.16406965e+00, -5.29558480e-01,\n        -1.45220721e+00,  1.66416144e+00,  2.15790653e+00,\n         3.38352728e+00,  1.02256703e+00,  1.10980594e+00,\n        -1.84015661e-01, -1.36797085e-01,  2.41913319e+00,\n        -5.34419107e+00,  3.40213227e+00, -3.93349171e+00,\n        -5.11542606e+00, -2.10441470e+00, -1.87723827e+00,\n        -1.27949250e+00, -1.52744293e-01, -6.91384494e-01,\n        -1.82883596e+00, -4.06754643e-01,  1.59101939e+00,\n         9.56900895e-01, -6.19212568e-01, -2.57471561e+00,\n        -5.13987601e-01,  2.79413104e+00,  1.56537557e+00,\n        -2.16655469e+00,  2.01541454e-01,  7.83449471e-01,\n         1.77123749e+00,  2.14171290e+00,  8.23073089e-01,\n         7.64191270e-01,  6.77583754e-01,  3.83488238e-02,\n         2.87841946e-01,  2.12866449e+00,  9.04804885e-01,\n        -1.27268147e+00,  1.29047477e+00, -6.52882695e-01,\n        -2.13121438e+00,  2.85688949e+00, -1.63898379e-01,\n         3.19513297e+00,  3.21907735e+00, -3.24092555e+00,\n        -1.95713747e+00,  2.72463632e+00,  1.90425205e+00,\n         4.16592687e-01,  7.15407014e-01, -1.41494572e+00,\n        -3.84007931e-01,  1.23961723e+00, -1.87443888e+00,\n         5.16553581e-01,  4.23543119e+00, -1.49012160e+00,\n         3.30235720e+00,  3.50617027e+00,  1.34791577e+00,\n         1.34893572e+00,  3.19949150e+00, -3.80942965e+00,\n        -1.89611399e+00,  2.24973249e+00, -2.71363497e+00,\n         2.60185575e+00, -3.79726553e+00, -7.34573841e-01,\n         2.93035269e+00,  5.14995396e-01,  2.80419040e+00,\n         1.93619514e+00,  3.13433141e-01,  7.71677315e-01,\n        -3.26780581e+00,  2.18355322e+00, -3.33139747e-01,\n         2.94758940e+00],\n       [ 1.82438183e+00,  1.01178598e+00,  4.55677181e-01,\n         2.78641880e-01, -3.62068266e-01, -2.30915263e-01,\n        -4.90694714e+00, -1.71707392e+00,  9.10058200e-01,\n         2.05802250e+00,  8.00583810e-02, -1.98811364e+00,\n        -1.43881768e-01, -2.42902756e+00, -1.16043699e+00,\n         7.67307103e-01,  1.26346856e-01, -2.18478584e+00,\n         9.03977871e-01,  3.30654383e-01, -2.01224947e+00,\n         6.46264315e-01,  2.14642525e+00, -2.00704551e+00,\n         8.57997596e-01,  1.98682308e+00,  2.41796851e+00,\n        -1.03253269e+00,  4.56734538e-01,  1.35914338e+00,\n         1.30586672e+00, -3.61663365e+00, -2.61804509e+00,\n         1.24707735e+00, -2.16980314e+00, -4.29357386e+00,\n        -1.76858795e+00, -2.84820628e+00, -5.30615759e+00,\n         1.74550086e-01,  6.43957436e-01, -3.96485448e+00,\n        -1.43089187e+00,  9.35435832e-01,  2.29839230e+00,\n         2.49195242e+00, -2.90075541e+00, -2.11428508e-01,\n         2.39097619e+00,  3.96145135e-01,  6.10250652e-01,\n        -4.71341515e+00, -1.92930901e+00,  8.44398662e-02,\n         3.37358534e-01, -1.62443042e+00,  1.53631938e+00,\n        -1.24341118e+00,  5.77365637e-01,  1.00069141e+00,\n         1.48519504e+00,  1.19979548e+00, -1.78181410e+00,\n        -4.36113453e+00,  1.84296668e+00, -1.92878532e+00,\n         9.59063172e-01, -1.39551198e+00,  6.89338326e-01,\n         2.86483854e-01,  7.62190998e-01,  1.03275728e+00,\n         8.79560113e-01,  1.72680843e+00,  4.33892161e-01,\n         2.16142321e+00, -5.09704232e-01, -4.38032717e-01,\n         2.80337691e+00,  1.91694343e+00, -9.84122396e-01,\n        -2.57574797e+00,  2.28641510e+00, -3.56642604e-01,\n        -1.92488742e+00,  1.90291822e+00, -1.08997071e+00,\n        -1.15260191e-01,  1.66098312e-01, -1.44641340e+00,\n        -1.83436632e+00, -4.60485190e-01, -9.54582214e-01,\n        -7.67206132e-01, -2.54850435e+00,  8.12341213e-01,\n        -1.57259107e+00,  2.23494148e+00, -1.15905547e+00,\n        -1.07092537e-01],\n       [ 2.80981008e-02, -6.08693957e-02, -2.62222528e-01,\n        -3.96138787e-01,  3.43412519e-01, -9.03649151e-01,\n        -7.74797946e-02,  4.12080735e-01, -6.60221279e-01,\n        -6.86456323e-01,  9.32549536e-01,  8.14801678e-02,\n         1.13970889e-02,  2.85199862e-02,  2.01695055e-01,\n        -3.40867639e-01, -5.07192791e-01,  1.34779990e-01,\n         4.87130165e-01, -8.91626000e-01,  2.82598019e-01,\n         5.77025175e-01,  9.12655950e-01, -2.76794404e-01,\n        -1.90092161e-01, -3.06071073e-01,  1.98501870e-01,\n         4.07807112e-01,  6.31286323e-01,  2.38942131e-02,\n         8.81404817e-01, -5.95110953e-01,  3.54817063e-01,\n        -8.74543667e-01, -4.90788400e-01,  9.11605716e-01,\n         3.67877811e-01, -4.57186103e-01, -8.21030661e-02,\n        -8.49701107e-01, -3.78125548e-01,  1.07296400e-01,\n        -1.13635719e+00,  1.34066129e-02,  5.33573866e-01,\n        -1.88287467e-01, -4.95788991e-01, -6.28350496e-01,\n         1.45966977e-01, -4.94848788e-01,  7.12107658e-01,\n         1.49470255e-01, -7.91468322e-01,  1.47463083e-01,\n         4.68267381e-01, -1.10477947e-01, -4.45721388e-01,\n        -6.41612887e-01, -5.17419875e-01, -2.93190509e-01,\n         8.52072760e-02,  3.24325502e-01,  1.30034339e+00,\n        -3.26764345e-01, -5.60511410e-01,  4.60569590e-01,\n        -7.57176638e-01,  3.59948814e-01,  1.29829511e-01,\n         7.11758196e-01,  7.43658021e-02, -7.62838960e-01,\n         3.37815791e-01,  1.25464988e+00,  3.48079503e-01,\n        -6.36199787e-02, -2.08449453e-01, -7.27347374e-01,\n        -9.40497994e-01, -8.44731987e-01, -9.20139104e-02,\n         3.55498433e-01, -6.06015682e-01,  1.78529787e+00,\n        -2.40345955e-01, -5.86390346e-02,  8.32722366e-01,\n         1.59079701e-01, -1.58611596e-01,  5.64258277e-01,\n        -2.92084098e-01, -8.10637511e-03,  2.77226031e-01,\n        -1.15494736e-01, -1.17794678e-01, -1.38564985e-02,\n         3.81056294e-02,  3.77161145e-01,  5.51310368e-02,\n        -5.74259996e-01],\n       [ 1.90056488e-01, -7.86895603e-02, -1.09026656e-01,\n         2.17535004e-01, -4.33847308e-03, -5.89419067e-01,\n         7.26781413e-02,  3.76719415e-01, -3.95884633e-01,\n        -2.36233130e-01,  2.52492994e-01, -1.63187787e-01,\n         3.13615769e-01, -1.52295902e-01, -9.55718085e-02,\n        -3.88845444e-01, -1.90588638e-01,  6.93082288e-02,\n         2.81355500e-01, -1.30249366e-01,  2.64313277e-02,\n         2.25900322e-01,  3.54196489e-01, -2.87849694e-01,\n         5.66374771e-02, -6.25240505e-02, -1.75715208e-01,\n         3.70377809e-01,  1.23973556e-01,  1.08199239e-01,\n         1.88092113e-01, -6.97698295e-01,  2.96097428e-01,\n        -4.79376107e-01, -5.17203324e-02, -1.91426992e-01,\n         1.23730458e-01, -2.62106925e-01, -2.05332398e-01,\n        -5.51851511e-01, -1.14394888e-01,  2.92462539e-02,\n        -5.38209856e-01,  1.75282493e-01,  5.70211649e-01,\n        -4.20944691e-01, -2.46441662e-01,  8.73412266e-02,\n         2.27761179e-01,  3.31554897e-02,  2.40155175e-01,\n         2.28743628e-01,  8.35675150e-02, -1.88041508e-01,\n        -2.60462239e-02,  2.69454718e-01,  1.99611783e-02,\n        -4.53628510e-01, -4.01159301e-02, -1.34375915e-01,\n        -8.16665813e-02, -1.34060323e-01,  5.10278046e-01,\n         2.26661727e-01, -1.78221941e-01,  4.79124963e-01,\n        -3.55087608e-01,  2.13360526e-02, -1.76620826e-01,\n         1.51178762e-01,  3.96896712e-02, -1.74217507e-01,\n         1.53373361e-01,  2.89137125e-01,  3.90357673e-01,\n        -1.89290360e-01,  2.54591435e-01, -6.08827293e-01,\n        -2.17860024e-02, -3.39331746e-01, -3.14778000e-01,\n        -1.44228622e-01,  2.93522716e-01,  6.43562078e-01,\n         1.29642174e-01,  1.84911072e-01,  4.06699181e-01,\n         2.82489598e-01, -1.76767752e-01,  3.71721715e-01,\n         2.63843983e-01, -7.03785494e-02,  2.42883891e-01,\n        -7.05613717e-02,  2.06910029e-01,  2.42428422e-01,\n         1.06940061e-01, -4.39303629e-02,  5.11761844e-01,\n        -1.05640464e-01],\n       [-4.76689190e-02, -2.24897563e-01, -7.07312644e-01,\n        -7.59628117e-01, -3.57922971e-01, -6.97716117e-01,\n        -4.09594089e-01,  7.04607248e-01, -6.79064870e-01,\n         4.69313949e-01,  7.07496583e-01,  2.12203767e-02,\n        -3.35362583e-01, -4.49312001e-01, -6.39839411e-01,\n        -7.83988386e-02, -8.76421213e-01, -5.60571134e-01,\n        -2.18524858e-02,  3.06285825e-02,  9.14923772e-02,\n         8.46070528e-01,  4.27049041e-01, -2.24293187e-01,\n        -3.41635458e-02, -8.75016525e-02,  1.68995738e-01,\n         5.47118247e-01,  7.20892131e-01,  1.14648128e-02,\n         4.62089092e-01, -4.18164343e-01, -7.41630718e-02,\n         3.55932951e-01, -1.74622014e-01,  8.11566353e-01,\n         3.53445143e-01, -1.38284898e+00, -4.35934067e-01,\n        -1.34356543e-01,  2.72670805e-01, -1.83058232e-02,\n        -1.52266049e+00,  9.47874710e-02,  2.88710922e-01,\n        -3.20556790e-01, -6.64498270e-01,  4.23299313e-01,\n        -4.86191094e-01,  2.57826149e-01,  1.14787543e+00,\n        -1.07582659e-01, -1.02661721e-01,  6.52082741e-01,\n        -8.40386003e-02,  2.40715533e-01, -1.69118613e-01,\n         3.32357377e-01,  1.73197567e-01, -4.37392980e-01,\n         1.56321064e-01,  2.05804676e-01,  7.29600072e-01,\n        -7.69666731e-01, -3.95795368e-02,  3.05820517e-02,\n        -4.98560339e-01, -9.37775850e-01, -6.72528446e-01,\n         5.55592030e-02,  4.61546779e-01, -1.40678152e-01,\n         1.42309377e-02, -3.23831797e-01, -2.16505364e-01,\n        -5.92948496e-02,  3.94055665e-01, -4.72197473e-01,\n         8.49576890e-02,  8.59966129e-03, -3.14948410e-01,\n        -2.98937768e-01,  3.63523245e-01,  3.03545654e-01,\n        -7.97763944e-01, -1.02255352e-01,  2.75987536e-01,\n         8.84967983e-01,  1.91284213e-02,  6.17895275e-02,\n        -3.63641024e-01, -1.88106209e-01,  6.93772733e-01,\n         6.49425834e-02, -3.18737417e-01,  5.76771319e-01,\n         1.59891937e-02, -2.83195521e-03,  1.76013276e-01,\n         2.44595602e-01],\n       [ 3.07292795e+00, -4.27944660e+00, -2.40080729e-02,\n        -1.35072172e+00,  1.27568579e+00, -4.31727934e+00,\n        -6.10438049e-01, -5.11173439e+00, -1.85431445e+00,\n        -2.24220681e+00,  1.31193721e+00,  2.88623667e+00,\n         8.65861535e-01,  6.28710794e+00,  8.31512988e-01,\n         2.09701490e+00, -1.51548791e+00,  3.06956816e+00,\n        -3.24062872e+00,  1.41908482e-01,  4.25161552e+00,\n         1.38190889e+00,  2.24758923e-01, -1.27173889e+00,\n         3.27435756e+00,  3.84649062e+00,  2.14656496e+00,\n         1.81496978e+00, -2.06279016e+00, -3.44561505e+00,\n        -1.73181212e+00,  8.80743489e-02,  2.46148086e+00,\n        -4.10392195e-01, -3.55808687e+00,  4.09590340e+00,\n        -9.98436391e-01, -1.79188037e+00,  2.04212213e+00,\n        -4.96141291e+00,  9.53869462e-01,  6.27949095e+00,\n        -2.12644982e+00, -1.59744883e+00, -3.10018921e+00,\n        -3.23911965e-01,  1.14515567e+00, -8.82410645e-01,\n         3.80917621e+00, -4.67356801e-01,  3.27654195e+00,\n         2.67216277e+00, -3.45974588e+00, -4.39715743e-01,\n         4.49092197e+00,  3.00118017e+00,  2.11902785e+00,\n         1.54358160e+00,  4.29328585e+00,  4.10572386e+00,\n         1.29496133e+00,  1.25961101e+00, -1.38619947e+00,\n         2.00065923e+00, -1.91689754e+00,  2.26082706e+00,\n        -2.98003435e+00,  5.72170973e-01, -5.07487679e+00,\n        -6.50965405e+00,  2.18508101e+00, -2.29291987e+00,\n        -1.95826161e+00, -1.20811963e+00,  4.46488500e-01,\n        -2.40511227e+00,  3.64895892e+00, -2.19341516e+00,\n        -7.65112460e-01, -7.74628282e-01, -7.14072526e-01,\n         3.48525643e+00, -1.54487050e+00, -1.55514747e-01,\n        -5.75759125e+00, -1.37040675e+00,  1.31622040e+00,\n        -2.66069365e+00,  1.24439585e+00, -2.14171910e+00,\n        -2.36260104e+00, -4.04785538e+00,  1.33854985e+00,\n         2.20817971e+00,  2.65394896e-01, -1.53422308e+00,\n         3.10177016e+00, -1.32944834e+00,  1.65588915e+00,\n        -2.74365830e+00],\n       [-3.50806909e-03, -2.49233055e+00, -6.26453757e-01,\n        -1.17105246e+00,  4.07632798e-01, -3.52097225e+00,\n         2.95250773e-01,  1.12666202e+00, -8.91805589e-01,\n        -1.61109105e-01,  3.57068509e-01, -8.17971885e-01,\n         3.52516234e-01, -1.46107388e+00, -3.48440468e-01,\n        -6.94134355e-01, -9.15686488e-01, -5.03300130e-01,\n         3.53972673e-01, -4.35176432e-01,  2.09309626e+00,\n         2.11297974e-01,  9.68319654e-01,  2.00079987e-03,\n         5.84100068e-01, -6.39832556e-01,  1.67744064e+00,\n         3.92483681e-01, -2.52156019e-01, -1.78543523e-01,\n         5.86690187e-01,  7.07521796e-01,  2.68825793e+00,\n        -5.06278515e-01, -7.11142793e-02, -3.68240088e-01,\n         1.24937065e-01, -1.95189261e+00,  1.76272448e-02,\n        -9.94432032e-01,  9.79018211e-01,  9.99852896e-01,\n         8.50123107e-01,  7.84877181e-01,  2.25739002e+00,\n         1.55867684e+00, -1.37331635e-01,  6.31552096e-03,\n        -9.86333787e-01, -4.76521440e-02, -9.18151140e-02,\n         5.60359716e-01, -1.05473411e+00, -6.00064099e-01,\n         4.84068036e-01, -7.12639034e-01, -8.40695500e-01,\n        -1.73587656e+00,  5.71701765e-01, -2.96842337e-01,\n         1.58156216e+00,  7.07778096e-01,  2.44847015e-01,\n        -7.54801273e-01,  3.27163070e-01,  2.54504585e+00,\n        -1.66743505e+00,  3.58869433e-01,  1.65651107e+00,\n        -8.63373220e-01, -1.29038677e-01, -1.00974071e+00,\n         3.57489794e-01, -1.09928750e-01,  8.93378109e-02,\n         2.00353789e+00, -3.75043988e-01, -1.36520791e+00,\n        -3.57408851e-01,  3.34796637e-01, -7.33272374e-01,\n        -3.54904234e-01, -8.08484316e-01,  2.88831443e-01,\n         1.41700506e-01, -2.38547862e-01,  1.26007009e+00,\n         1.58065271e+00, -2.31196702e-01,  8.17291617e-01,\n        -1.18835068e+00, -5.02209067e-01,  1.08549261e+00,\n        -1.01023233e+00, -4.14879441e-01,  7.83213437e-01,\n         6.42816961e-01,  1.25290370e+00, -8.33656490e-01,\n         4.89352077e-01],\n       [-1.97414815e+00, -2.08014750e+00, -4.35460138e+00,\n        -1.76642942e+00, -1.53961456e+00, -1.99800909e+00,\n        -3.62172753e-01, -3.44314361e+00,  7.77279973e-01,\n         3.06916165e+00,  7.56313503e-01,  7.10521400e-01,\n        -2.92390442e+00, -1.85470951e+00, -8.30788553e-01,\n         1.64393568e+00,  9.99232888e-01, -1.20064676e+00,\n         1.98941529e+00, -7.18053758e-01, -8.71736884e-01,\n         1.71759617e+00,  3.04983950e+00, -3.96784377e+00,\n        -6.22513771e-01, -4.58402967e+00,  1.89707887e+00,\n         6.58023977e+00,  3.70037889e+00, -1.80167294e+00,\n         2.54874706e+00,  1.48546338e+00,  4.41209853e-01,\n         2.41420364e+00, -3.18446904e-02,  2.96437812e+00,\n        -9.16070521e-01, -3.74440312e+00, -3.99818420e+00,\n        -3.65947261e-02, -4.54862165e+00, -9.16280270e-01,\n        -1.99805796e+00,  5.31040251e-01,  7.16196477e-01,\n         2.54742742e-01, -2.59655213e+00, -7.30355307e-02,\n        -4.85127836e-01,  9.10672367e-01,  1.19877434e+00,\n         1.72432792e+00, -1.89069426e+00,  1.76178992e+00,\n         2.57992315e+00,  4.85537313e-02, -7.41672277e-01,\n        -2.92255735e+00, -7.18405485e-01, -2.79012060e+00,\n         1.65325117e+00,  1.52133942e+00, -6.60335049e-02,\n        -6.24072969e-01,  3.28167653e+00,  1.25756431e+00,\n        -2.86892509e+00, -3.43954897e+00,  1.01302207e+00,\n         3.42199850e+00,  1.87994945e+00, -9.60003257e-01,\n         1.41227007e-01, -2.33887291e+00, -1.16124499e+00,\n        -1.47701025e+00, -5.25018156e-01, -2.69220543e+00,\n         2.00584745e+00,  4.74036026e+00,  2.28879523e+00,\n        -4.14333296e+00,  3.83907795e+00,  8.36375833e-01,\n        -3.52072835e+00,  1.38560688e+00, -5.32426882e+00,\n         1.91933119e+00,  7.71074653e-01,  8.74321997e-01,\n        -3.25264001e+00,  1.88503122e+00,  2.32145834e+00,\n        -1.95003080e+00, -2.97816753e+00,  2.26926064e+00,\n         3.85743928e+00, -5.14129210e+00,  1.05667174e+00,\n         1.16770530e+00],\n       [ 1.26766753e+00, -4.10106599e-01, -4.91342485e-01,\n         8.50624204e-01,  1.92864978e+00, -2.84847093e+00,\n         2.43462348e+00,  9.01059866e-01, -5.77257454e-01,\n        -3.02241206e+00,  2.72427320e-01, -3.00413996e-01,\n         5.70276976e-01, -1.14953649e+00,  7.34179541e-02,\n        -2.12466812e+00, -7.84146249e-01,  2.16892171e+00,\n         3.60600352e-01, -1.48577258e-01,  5.50990403e-01,\n        -1.18904579e+00, -8.95085871e-01,  4.32324618e-01,\n         3.19883019e-01,  8.47482800e-01, -2.85376859e+00,\n        -3.28850657e-01, -2.96241455e-02, -1.47424054e+00,\n         1.29293585e+00, -2.51832271e+00,  2.25338292e+00,\n        -7.08164334e-01,  6.28097475e-01,  1.14518917e+00,\n        -3.47820848e-01,  1.25961316e+00,  4.37742382e-01,\n        -4.90817696e-01, -1.67763746e+00,  8.89551759e-01,\n         1.42737401e+00,  4.83176231e-01,  1.01385844e+00,\n         6.86399817e-01, -1.34195602e+00, -3.70169312e-01,\n        -1.19087291e+00, -5.91293633e-01,  1.15030253e+00,\n        -1.59415329e+00, -4.54670280e-01,  1.99464157e-01,\n         6.90210283e-01, -5.12360334e-01,  2.51354545e-01,\n        -7.92508781e-01, -7.29948938e-01,  2.05408669e+00,\n         5.22676826e-01, -9.95498598e-02,  2.60528731e+00,\n        -9.31807458e-01,  3.88682723e-01, -2.01153591e-01,\n         5.83715200e-01,  4.14001524e-01, -2.17664745e-02,\n        -2.40407681e+00,  2.54086643e-01,  3.87810647e-01,\n        -1.93427831e-01,  8.63233030e-01,  9.84679043e-01,\n        -2.64071703e+00, -1.33322108e+00,  9.28071499e-01,\n        -4.92453247e-01, -1.07953203e+00, -6.23802423e-01,\n         4.34693396e-01, -1.18897986e+00,  1.96509612e+00,\n        -5.98912001e-01,  1.15473771e+00,  1.32225776e+00,\n         8.19093406e-01, -2.59653866e-01, -5.36838889e-01,\n         2.31268972e-01, -5.89061260e-01, -1.34604543e-01,\n        -4.12822455e-01,  1.30070710e+00, -6.38021588e-01,\n        -2.24307728e+00,  3.54329437e-01,  1.45006824e+00,\n        -1.27595448e+00]], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Creating Training Features and Labels(Next Word) \n#### Training_Features --> (Previous n-1 Words)\n#### Labels --> (Next Word or, n-th Word)","metadata":{}},{"cell_type":"code","source":"# for text generation it will be\n# source : [[1,2],[1,2,3],[1,2,3,4],[1,2,3,4,5]]\n# x --------------------------------y\n# 1                                 2\n# 1,2                               3\n# 1,2,3                             4\n# 1,2,3,4                           5\nX = []\nY = []\n\nfor seq in data_set:\n    X.append(seq[:-1])\n    Y.append(seq[-1])","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:08.501223Z","iopub.execute_input":"2024-03-07T21:14:08.501942Z","iopub.status.idle":"2024-03-07T21:14:09.377862Z","shell.execute_reply.started":"2024-03-07T21:14:08.501909Z","shell.execute_reply":"2024-03-07T21:14:09.377094Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Some CUDA Memory Management Techniques \n### (Can be useful for Large Datsets and Variable length Padding)","metadata":{}},{"cell_type":"code","source":"# ****Collecting garbages to free up memory****\n\nimport gc\n#del variable_name\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:09.702968Z","iopub.execute_input":"2024-03-07T21:14:09.703613Z","iopub.status.idle":"2024-03-07T21:14:09.946768Z","shell.execute_reply.started":"2024-03-07T21:14:09.703580Z","shell.execute_reply":"2024-03-07T21:14:09.945829Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"import os\n\n# all Pytorch Cuda parameters will be reset.. we may use this for CudaOutOfMemory Problem\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = ''","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:10.121648Z","iopub.execute_input":"2024-03-07T21:14:10.121964Z","iopub.status.idle":"2024-03-07T21:14:10.126404Z","shell.execute_reply.started":"2024-03-07T21:14:10.121935Z","shell.execute_reply":"2024-03-07T21:14:10.125462Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# to check if GPU is available or not\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:11.136958Z","iopub.execute_input":"2024-03-07T21:14:11.137599Z","iopub.status.idle":"2024-03-07T21:14:14.596995Z","shell.execute_reply.started":"2024-03-07T21:14:11.137564Z","shell.execute_reply":"2024-03-07T21:14:14.595987Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# to check which gpu is running our code.. usefull for multi gpu environment\ntorch.cuda.current_device()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:14.598856Z","iopub.execute_input":"2024-03-07T21:14:14.599354Z","iopub.status.idle":"2024-03-07T21:14:14.626027Z","shell.execute_reply.started":"2024-03-07T21:14:14.599328Z","shell.execute_reply":"2024-03-07T21:14:14.625135Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# used to empty cache from the memory..\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:14.627124Z","iopub.execute_input":"2024-03-07T21:14:14.627381Z","iopub.status.idle":"2024-03-07T21:14:14.631513Z","shell.execute_reply.started":"2024-03-07T21:14:14.627358Z","shell.execute_reply":"2024-03-07T21:14:14.630629Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# get gpu summary..\n\n# torch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:14.633801Z","iopub.execute_input":"2024-03-07T21:14:14.634316Z","iopub.status.idle":"2024-03-07T21:14:14.641424Z","shell.execute_reply.started":"2024-03-07T21:14:14.634284Z","shell.execute_reply":"2024-03-07T21:14:14.640531Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# set maximum allowed processes..\n\n# torch.set_num_threads(4)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:14.642556Z","iopub.execute_input":"2024-03-07T21:14:14.642855Z","iopub.status.idle":"2024-03-07T21:14:14.651368Z","shell.execute_reply.started":"2024-03-07T21:14:14.642825Z","shell.execute_reply":"2024-03-07T21:14:14.650655Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# check for GPU conditions like Temp., core usage, Utilization, Memory usage, Running processses etc. \n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:14.652326Z","iopub.execute_input":"2024-03-07T21:14:14.652635Z","iopub.status.idle":"2024-03-07T21:14:15.710926Z","shell.execute_reply.started":"2024-03-07T21:14:14.652612Z","shell.execute_reply":"2024-03-07T21:14:15.709622Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Thu Mar  7 21:14:15 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0              25W / 250W |      2MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# This will reset all GPU states... can help in CudaOutOfMemory problem.\n\n# !nvidia-smi --gpu-reset\n# !nvidia-smi --gpu-reset -i 0","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:15.712487Z","iopub.execute_input":"2024-03-07T21:14:15.712803Z","iopub.status.idle":"2024-03-07T21:14:15.717875Z","shell.execute_reply.started":"2024-03-07T21:14:15.712774Z","shell.execute_reply":"2024-03-07T21:14:15.716827Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Taking Data into GPU and After Padding","metadata":{}},{"cell_type":"code","source":"# for Taking Data into GPU...\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# device","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:15.720903Z","iopub.execute_input":"2024-03-07T21:14:15.721544Z","iopub.status.idle":"2024-03-07T21:14:15.728994Z","shell.execute_reply.started":"2024-03-07T21:14:15.721508Z","shell.execute_reply":"2024-03-07T21:14:15.727985Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# torch.tensor(data_set) will fail because each element in the 2d array is of different length..\n\n# torch.tensor(data_set[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:15.730065Z","iopub.execute_input":"2024-03-07T21:14:15.730351Z","iopub.status.idle":"2024-03-07T21:14:15.739477Z","shell.execute_reply.started":"2024-03-07T21:14:15.730328Z","shell.execute_reply":"2024-03-07T21:14:15.738591Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Coverting to tensor for n-grams of variable length.. not recommended for large text corpus\n\n# import numpy as np\n# import torch\n\n# tensor_list=[]\n# for x in X:\n#     tensor_list.append(torch.tensor(x, dtype=torch.float32))\n# print(len(tensor_list))\n\n# tensor_Y = torch.tensor(Y, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:15.848160Z","iopub.execute_input":"2024-03-07T21:14:15.848450Z","iopub.status.idle":"2024-03-07T21:14:15.852545Z","shell.execute_reply.started":"2024-03-07T21:14:15.848425Z","shell.execute_reply":"2024-03-07T21:14:15.851559Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# tensor_list[0].shape, tensor_list[1].shape, tensor_list[2].shape, tensor_list[3].shape, ","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:16.095095Z","iopub.execute_input":"2024-03-07T21:14:16.095505Z","iopub.status.idle":"2024-03-07T21:14:16.099976Z","shell.execute_reply.started":"2024-03-07T21:14:16.095473Z","shell.execute_reply":"2024-03-07T21:14:16.098964Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# ****** Padding needs for n-grams ofvariable length..******\n\n# Find the maximum length of tensors in the list..\n# max_length = max(tensor.size(0) for tensor in tensor_list)\n\n# Pad tensors to the maximum length..\n# padded_tensors = []\n\n# for tensor in tensor_list:\n#     padded_tensors.append(torch.cat((tensor.to(device), torch.zeros(max_length - tensor.size(0), tensor.size(1)).to(device))))\n\n# chunk_size = 100\n# for i in range(0, len(tensor_list), chunk_size):\n#     chunk = tensor_list[i:i+chunk_size]\n#     for tensor in chunk:\n#          padded_tensors.append(torch.cat((tensor.to(device), torch.zeros(max_length - tensor.size(0), tensor.size(1)).to(device))))\n\n    # if any meamory_leak happens, it will be handeled here..  however process will become slow..\n#     torch.cuda.empty_cache()\n#     del chunk\n#     gc.collect()\n\n\n# Concatenate padded tensors along dimension 0 i.e. row-wise..\n# tensor_X = torch.stack(padded_tensors, dim=0)\n# tensor_y = tensor_Y.to(device) \n\n# don't forget to clear the list(not required anymore) : 'tensor_list' & 'padded_tensors'\n# del tensor_list\n# del padded_tensors \n# gc.collect()\n\n\n# tensor_X.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:16.284015Z","iopub.execute_input":"2024-03-07T21:14:16.284309Z","iopub.status.idle":"2024-03-07T21:14:16.289361Z","shell.execute_reply.started":"2024-03-07T21:14:16.284284Z","shell.execute_reply":"2024-03-07T21:14:16.288537Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# for small dataset we can use below code (for large it may show CudaOutOfMemory error)..\n\n# from torch.nn.utils.rnn import pad_sequence\n\n# # Pad the sequence\n# padded_X = pad_sequence(tensor_X, padding_value=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:16.475184Z","iopub.execute_input":"2024-03-07T21:14:16.475496Z","iopub.status.idle":"2024-03-07T21:14:16.479082Z","shell.execute_reply.started":"2024-03-07T21:14:16.475470Z","shell.execute_reply":"2024-03-07T21:14:16.478381Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Taking Data into GPU and Creating tensors","metadata":{}},{"cell_type":"code","source":"# for Taking Data into GPU...\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:18.284179Z","iopub.execute_input":"2024-03-07T21:14:18.284554Z","iopub.status.idle":"2024-03-07T21:14:18.291014Z","shell.execute_reply.started":"2024-03-07T21:14:18.284523Z","shell.execute_reply":"2024-03-07T21:14:18.290123Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"tensor_X = torch.tensor(X, dtype=torch.float32).to(device)\ntensor_Y = torch.tensor(Y, dtype=torch.float32).to(device)\n\ntensor_X.shape, tensor_Y.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:14:19.092991Z","iopub.execute_input":"2024-03-07T21:14:19.093716Z","iopub.status.idle":"2024-03-07T21:18:33.563554Z","shell.execute_reply.started":"2024-03-07T21:14:19.093680Z","shell.execute_reply":"2024-03-07T21:18:33.562644Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/219624731.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  tensor_X = torch.tensor(X, dtype=torch.float32).to(device)\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"(torch.Size([886850, 9, 100]), torch.Size([886850, 100]))"},"metadata":{}}]},{"cell_type":"code","source":"# if any meamory_leak happens, it will be handeled here..\n\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:53.434340Z","iopub.execute_input":"2024-03-07T21:20:53.434775Z","iopub.status.idle":"2024-03-07T21:20:53.717925Z","shell.execute_reply.started":"2024-03-07T21:20:53.434740Z","shell.execute_reply":"2024-03-07T21:20:53.716979Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# now we can delete all variables stored on CPU.. but it's recommended only_if we have no space in RAM..\n# however the variables 'X' and 'Y' can be deleted as they are slices of original vector 'seq' \n# and can be retrieved if necessary\n\n# del X\n# del Y\n\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:54.150798Z","iopub.execute_input":"2024-03-07T21:20:54.151109Z","iopub.status.idle":"2024-03-07T21:20:54.157378Z","shell.execute_reply.started":"2024-03-07T21:20:54.151085Z","shell.execute_reply":"2024-03-07T21:20:54.156484Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Spliting Data into Train and Test Sets","metadata":{}},{"cell_type":"code","source":"length = tensor_X.shape[0]\nlength","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:55.037600Z","iopub.execute_input":"2024-03-07T21:20:55.038280Z","iopub.status.idle":"2024-03-07T21:20:55.044060Z","shell.execute_reply.started":"2024-03-07T21:20:55.038250Z","shell.execute_reply":"2024-03-07T21:20:55.043164Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"886850"},"metadata":{}}]},{"cell_type":"code","source":"train_split = int(length * 0.9)\ntest_split = int(length * 0.1)\n\ntrain_split, test_split","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:55.479891Z","iopub.execute_input":"2024-03-07T21:20:55.480191Z","iopub.status.idle":"2024-03-07T21:20:55.486392Z","shell.execute_reply.started":"2024-03-07T21:20:55.480165Z","shell.execute_reply":"2024-03-07T21:20:55.485440Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"(798165, 88685)"},"metadata":{}}]},{"cell_type":"code","source":"X_train = tensor_X[:train_split]\nX_test = tensor_X[train_split:]\n\ny_train = tensor_Y[:train_split]\ny_test = tensor_Y[train_split:]","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:56.115887Z","iopub.execute_input":"2024-03-07T21:20:56.116203Z","iopub.status.idle":"2024-03-07T21:20:56.120713Z","shell.execute_reply.started":"2024-03-07T21:20:56.116176Z","shell.execute_reply":"2024-03-07T21:20:56.119801Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:56.870306Z","iopub.execute_input":"2024-03-07T21:20:56.870928Z","iopub.status.idle":"2024-03-07T21:20:56.876784Z","shell.execute_reply.started":"2024-03-07T21:20:56.870897Z","shell.execute_reply":"2024-03-07T21:20:56.875854Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"(torch.Size([798165, 9, 100]),\n torch.Size([88685, 9, 100]),\n torch.Size([798165, 100]),\n torch.Size([88685, 100]))"},"metadata":{}}]},{"cell_type":"code","source":"y_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:57.286926Z","iopub.execute_input":"2024-03-07T21:20:57.287746Z","iopub.status.idle":"2024-03-07T21:20:57.298399Z","shell.execute_reply.started":"2024-03-07T21:20:57.287713Z","shell.execute_reply":"2024-03-07T21:20:57.297452Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"tensor([ 1.2677, -0.4101, -0.4913,  0.8506,  1.9286, -2.8485,  2.4346,  0.9011,\n        -0.5773, -3.0224,  0.2724, -0.3004,  0.5703, -1.1495,  0.0734, -2.1247,\n        -0.7841,  2.1689,  0.3606, -0.1486,  0.5510, -1.1890, -0.8951,  0.4323,\n         0.3199,  0.8475, -2.8538, -0.3289, -0.0296, -1.4742,  1.2929, -2.5183,\n         2.2534, -0.7082,  0.6281,  1.1452, -0.3478,  1.2596,  0.4377, -0.4908,\n        -1.6776,  0.8896,  1.4274,  0.4832,  1.0139,  0.6864, -1.3420, -0.3702,\n        -1.1909, -0.5913,  1.1503, -1.5942, -0.4547,  0.1995,  0.6902, -0.5124,\n         0.2514, -0.7925, -0.7299,  2.0541,  0.5227, -0.0995,  2.6053, -0.9318,\n         0.3887, -0.2012,  0.5837,  0.4140, -0.0218, -2.4041,  0.2541,  0.3878,\n        -0.1934,  0.8632,  0.9847, -2.6407, -1.3332,  0.9281, -0.4925, -1.0795,\n        -0.6238,  0.4347, -1.1890,  1.9651, -0.5989,  1.1547,  1.3223,  0.8191,\n        -0.2597, -0.5368,  0.2313, -0.5891, -0.1346, -0.4128,  1.3007, -0.6380,\n        -2.2431,  0.3543,  1.4501, -1.2760], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Creating and Loading Datasets from Raw Tensors","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, Dataset, DataLoader\n\n# writing custom dataset function \nclass Dataset(Dataset):\n    def __init__(self, features, labels):\n        # if tensor is passes\n        self.features = features\n        self.labels = labels\n        \n        # if numpy array are passed\n#         self.features = torch.tensor(features, dtype=torch.float32)\n#         self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:57.853486Z","iopub.execute_input":"2024-03-07T21:20:57.854174Z","iopub.status.idle":"2024-03-07T21:20:57.860095Z","shell.execute_reply.started":"2024-03-07T21:20:57.854141Z","shell.execute_reply":"2024-03-07T21:20:57.859103Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Create TensorDatsets..\ntrain_data = TensorDataset(X_train, y_train)\ntest_data = TensorDataset(X_test, y_test)\n\n# We may use our custom dataset..\n# train_data = CustomDataset(X_train, y_train)\n# train_data = CustomDataset(X_train, y_train)\n\n\n# Create a DataLoader to iterate over the dataset\ntrain_loader = DataLoader(train_data, batch_size=256, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=256, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:58.088307Z","iopub.execute_input":"2024-03-07T21:20:58.088585Z","iopub.status.idle":"2024-03-07T21:20:58.093931Z","shell.execute_reply.started":"2024-03-07T21:20:58.088562Z","shell.execute_reply":"2024-03-07T21:20:58.092957Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# printing first 10 inputs and labels shape.. from the dataloader\ni = 0\nfor (inputs, labels) in train_loader:\n    print(\"INPUT : \", inputs.shape)\n    print(\"Label : \", labels.shape, \"\\n\\n\")\n\n    i += 1\n    \n    if(i==10):\n        break","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:20:58.307346Z","iopub.execute_input":"2024-03-07T21:20:58.307707Z","iopub.status.idle":"2024-03-07T21:20:58.430566Z","shell.execute_reply.started":"2024-03-07T21:20:58.307678Z","shell.execute_reply":"2024-03-07T21:20:58.429370Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"INPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\nINPUT :  torch.Size([256, 9, 100])\nLabel :  torch.Size([256, 100]) \n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Cnclusion :\n\n#### Now we are ready for passing the loader into our Language Modle.\n##### As It was a Text Preprocessing Tutorial, so I am not diving into Building and Trainning Language models for now. \n\n### THANK YOU","metadata":{}},{"cell_type":"markdown","source":"# References\n- [https://medium.com/analytics-vidhya/bengali-text-visualization-using-word2vec-211e2ed9fa30](https://medium.com/analytics-vidhya/bengali-text-visualization-using-word2vec-211e2ed9fa30)\n\n- [https://www.kaggle.com/code/asif00/text-generation-with-tensorflow-nlp-rnn/notebook](https://www.kaggle.com/code/asif00/text-generation-with-tensorflow-nlp-rnn/notebook)\n\n","metadata":{}}]}